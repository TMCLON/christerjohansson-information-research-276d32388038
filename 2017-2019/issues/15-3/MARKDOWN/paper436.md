#### vol. 15 no. 3, September, 2010



# Standardised library instruction assessment: an institution-specific approach

#### [Shannon M. Staley](#authors)  
Dr. Martin Luther King, Jr. Library, San Jose State University, One Washington Square, San Jose, CA 95192-0028, United States

#### [Nicole A. Branch](#authors) and [Tom L. Hewitt](#authors)  
School of Library and Information Science, San Jose State University, One Washington Square, San Jose, CA 95192-0029, United States

#### Abstract

> **Introduction.** We explore the use of a psychometric model for locally-relevant, information literacy assessment, using an online tool for standardised assessment of student learning during discipline-based library instruction sessions.  
> **Method**. A quantitative approach to data collection and analysis was used, employing standardised multiple-choice survey questions followed by individual, cognitive interviews with undergraduate students. The assessment tool was administered to five general education psychology classes during library instruction sessions.  
> **Analysis**. Descriptive statistics were generated by the assessment tool.  
> **Results**. The assessment tool proved a feasible means of measuring student learning. While student scores improved on every survey question, there was uneven improvement from pre-test to post-test for different questions.  
> **Conclusion**. Student scores showed more improvement for some learning outcomes over others, thus, spending time on fewer concepts during instruction sessions would enable more reliable evaluation of student learning. We recommend using digital learning objects that address basic research skills to enhance library instruction programmes. Future studies will explore different applications of the assessment tool, provide more detailed statistical analysis of the data and shed additional light on the significance of overall scores.

## Introduction

### Assessment in the library science field

In the library science field, assessment is an ongoing and essential component to information literacy instruction, although it can be difficult to put into effective practice. In 2003, the Association of College and Research Libraries approved guidelines for information literacy best practices, which emphasize ten categories of instructional efficacy, including assessment and evaluation ([American Library Association 2003](#ala03)). In addition to measurable assessment, the Association's best practices ([American Library Association 2006a](#ala06a)) outline the need for student learning outcomes that mirror the performance indicators widely accepted by the profession's Information Literacy Competency Standards for Higher Education ([American Library Association 2006b](#ala06b)). While most academic librarians agree that assessment is important, there is lack of consistency when it comes to implementation ([Donovan and Winterman 2009](#don09)). However, particularly in the current budgetary climate, a uniform approach to assessment is critical to improving the instructional programme and providing a systematic basis for institutional support.

### Assessment at San Jose State University

San Jose State University administrators and disciplinary faculty ascribe great importance to information literacy instruction as is evidenced by the University Academic Senate's recommendations ([San Jose State University Academic Senate 2004](#sjsuas04)), the mission and shared values of the university ([San Jose State University 2007](#sjsu07)) and its strategic learning goals ([San Jose State University Library 2007](#sjsul07)). The university's assessment programme is further guided by the Western Association of Schools and Colleges (WASC) criteria for review during the accreditation process ([Western Association of Schools and Colleges 2008](#wasc08)). Consequently, an evidence-based approach to instruction is not only helpful to inform effective teaching models, but is required to maintain the integrity of disciplinary programmes across campus ([Bogel 2008](#bog08)).

Currently, there is no consistent or uniform way of assessing student information literacy skills at San Jose State University. While academic librarians assess students' comprehension of information literacy concepts in a number of different ways, multiple-choice tests, or surveys, are common tools ([Williams 2000](#wil00)). However, a review of the literature reveals that many survey instruments in the field are created without a process of standardisation to yield statistically sound data ([Cameron _et al._ 2007](#cam07)). Moreover, most that are standardised provide large-scale, general measures of student information literacy competence. Such surveys, consequently, lack ties to institution-specific learning outcomes. This begs the question: how can librarians develop a locally-relevant, psychometric tool to assess student learning given the time and resource constraints of the current academic environment? In addressing this question, the authors explored the use of an online survey assessment tool that could measure student learning at San Jose State University.

## Background and literature review

### Survey assessment tools

There is much literature on the use of surveys as assessment tools. Allen and Babbie note that '_survey research is perhaps the most frequently used mode of observation in the social sciences_' ([Allen and Babbie 2008](#all08): 366). In the library and information science field, the [Multimedia Educational Resource for Learning and Online Teaching](http://www.webcitation.org/5oneD3OGA) ([2009](#mer09)) site offers a number of survey assessment tools for educators. Additionally, Merz and Mark ([2002](#mer02)) have compiled examples of information literacy assessment instruments developed by librarians at different higher education institutions for shared learning opportunities.

### Pre-test and post-test surveys

Many studies have shown that pre- and post-tests are a successful means of measuring library instructional efficacy at the institution level. Koehler and Swanson ([1988](#koe88)) worked with international students over a three-year period using such an approach. This longitudinal study employed the use of pre-tests to assess student readiness and post-tests to measure changes in information literacy comprehension over the study's duration. The authors reported a vast increase in student competence levels. Similarly, Knight's ([2002](#kni02)) use of pre- and post-tests during student instruction resulted in student improvement on every question relating to library research skills. In another study, Jackson ([2006](#jac06)) noted a six percent improvement in student comprehension of plagiarism concepts through an analysis of pre-and post-test scores related to an online tutorial.

### Standardised surveys

At present, few authors have used standardised, local instruments to determine whether student respondents interpret survey questions correctly. Non-standardised questions run the risk of being confusing, misleading, or biased. Without appropriate methods of developing assessment instruments, test score improvement as an indicator of student learning is called into question. Cameron _et al_. note that '_there is a need for reliable and valid data on student learning outcomes_' ([Cameron _et al_. 2007](#cam07): 230).

A few studies have applied survey assessment tools that have been expertly evaluated. Gilstrap and Dupree ([2008](#gil08)) describe the use of Brookfield's Critical Incident Questionnaire ([1995](#bro95)), a tool that prompts students to write open-ended responses to critical incidents, or moments of learning recognition, they experience during an instruction session. In another study, James Madison University librarians and faculty developed the Information-Seeking Skills Test, a discipline-based, online survey instrument for measuring student information literacy competence at the first-year level ([Cameron _et al_. 2007](#cam07)). Moreover, at Central Michigan University, librarians used pre- and post-tests to measure the impact of library instruction on students' information literacy skills through the use of the Research Readiness Self-Assessment tool, which was evaluated for validity and reliability ([Mathson and Lorenzen 2008](#mat08)).

### Large-scale surveys

While there are a few standardised instruments useful at the institution level, most are developed as tests of general information literacy skills geared towards the larger academic community. The Research Readiness Self-Assessment tool, originally developed for local institutional needs, has now been expanded for worldwide application by other academic institutions, whose specific needs may or may not be met ([Mathson and Lorenzen 2008](#mat08)). Similarly, James Madison University educators evolved their local Information-Seeking Skills Test tool for use by other academic institutions as the Information Literacy Test ([Cameron _et al_. 2007](#cam07)). Blixrud ([2003](#bli03)) explains that the Standard Assessment of Information Literacy Skills test measures data on student information literacy skills at academic institutions on a national level. The tool is further described as one that '_contains items not specific to a particular institution or library_' ([Kent State University 2008](#ken08): para. 3). The Tool for Real-Time Assessment of Information Literacy developed by the Institute for Library and Information Literacy Education and Kent State University Libraries, is a free, online survey instrument used to assess '_skills and concepts generally considered essential to information literacy_' ([Schloman and Gedeon 2007](#sch07): 2). Additionally, the Information and Communication Technology Literacy Test ([Kenney 2006](#ken06)), now called _iSkills_, is a large-scale test ([Rockman and Smith 2005](#roc05)) for purchase to assess students' general competency in information and communication technology ([Educational Testing Service 2009](#ets09)) through the replication of real-world, online tasks ([Somerville _et al._ 2008](#som08)).

### The need for a localised, psychometric tool

The California State University has endorsed _iSkills_ as a tool for measuring student information communication technology proficiency ([California State University 2007](#csu07)). However, as the University's Information/ICT Literacy Strategic Planning Committee has acknowledged the need for assessment at '_systemwide and campus levels_,' a more precise method of measuring student learning at the local, San Jose State level is necessary ([California State University [n.d.]](#csund): 2). A thoughtfully-designed, local tool could collectively address national, campus and departmental assessment standards in a cost-effective manner that more accurately identifies areas for programmatic improvement at the University Library.

## Method

To investigate this issue, an online application allowing librarians to select expertly-evaluated, multiple-choice questions for use during library instruction sessions was developed. The tool generates automated pre- and post-test surveys containing questions that match the specific learning outcomes of particular courses across campus disciplines at San Jose State University (see Table 1 for learning outcomes applied in this study). Using this tool, the authors launched a case study whose findings will be analysed in greater depth during future studies.

Specifically, the authors explored a programmatic model for information literacy assessment. This involved the development of standardised, multiple-choice questions and entering them into the online assessment tool. An important piece of this process entailed gauging whether it was feasible to use the tool during one-off, library instruction sessions and if so, to gain some preliminary insight from test score comparisons:

*   Do student survey scores increase after a library instruction intervention?
*   What kinds of programme improvements, if any, do student scores suggest?

### Survey research design

The authors chose a pre- and post- test survey research design to discern differences in student scores before and after a library instruction session. The intent was to gain measures of formative and summative assessment (evaluation of student achievement as well as instructional efficacy on the part of librarians) in making decisions about programme improvement ([The Center for Effective Teaching and Learning at the University of Texas at El Paso [n.d.]](#texnd))

Pre- and post-test surveys were created from a pool of multiple-choice questions covering various information literacy concepts in the social sciences, including citation analysis, identification of scholarly sources, appropriate subject databases and library Web site navigation ([see Appendix A](#appA)).

Psychology falls within the broader area of social sciences and is a part of the College of Social Sciences at San Jose State University. Consequently, the 11 questions were developed based on learning outcomes identified by a team of librarians specializing in the social sciences at San Jose State University. These librarians worked together to formulate information literacy learning outcomes reflecting general education needs across the social sciences at San Jose State University. Reference statistics, from both the Main Reference Desk and in-person consultations, also influenced the process. For future studies, the authors plan to customize survey questions for particular subject areas whenever possible. Ultimately, all five junior-level psychology courses received the same eleven survey questions testing information literacy skills. See Table 1, below, for more details.

Additionally, all surveys administered to students in junior-level psychology courses contained six background questions before the eleven testing students' knowledge of research skills. These questions were meant to provide descriptive statistics and, through future studies, identify variables that might affect a student's incoming research experience independent of the actual instruction session. Background questions included whether students had received library instruction before, how often they conducted research at the library (both in person and remotely), student class level and student major, among others (see [Appendix A](#appA)).

### Question development

The aforementioned questions were entered into the online assessment tool. The creation of survey questions involved consultation with San Jose State University librarians, existing information literacy tutorials and survey instruments such as the University of Texas Information Literacy Tutorial adapted by various other universities ([1998](#tilt98)). Each question was linked to applicable Association of College and Research Libraries standards and performance indicators ([American Library Association 2006b](ala06b)), American Psychological Association undergraduate learning goals ([American Psychological Association 2007](apa07)) and social sciences learning outcomes developed by San Jose State Univerity librarians for general education courses. See Table 1 for more details.

<table width="100%" border="1" cellspacing="0" cellpadding="6" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd"><caption align="bottom">  
**Table 1\. Learning outcomes, Association of College and Research Libraries performance indicators and American Psychological Association learning goals covered by survey questions**</caption>

<tbody>

<tr>

<th>Question</th>

<th>Social Sciences Learning Outcomes</th>

<th>ACRL Standards / Performance Indicators</th>

<th>APA Learning Goals</th>

</tr>

<tr>

<td valign="top">**1.** Imagine you have an assignment to write a paper based on scholarly information. Which would be the most appropriate source to use?</td>

<td valign="top">Understand the difference between popular and scholarly literature</td>

<td valign="top">Articulate and apply initial criteria for evaluating both the information and its sources</td>

<td valign="top">Use selected sources after evaluating their suitability based on appropriateness, accuracy, quality, and value of the source</td>

</tr>

<tr>

<td valign="top">**2.** How can you tell you are reading a popular magazine?</td>

<td valign="top">Understand the difference between popular and scholarly literature</td>

<td valign="top">Articulate and apply initial criteria for evaluating both the information and its sources</td>

<td valign="top">Use selected sources after evaluating their suitability based on appropriateness, accuracy, quality, and value of the source</td>

</tr>

<tr>

<td valign="top">**3.** What is the name of the linking tool found in SJSU databases that may lead you to the full text of an article?</td>

<td valign="top">Determine local availability of cited item and use Link+ and interlibrary loan services as needed</td>

<td valign="top">Determine the availability of needed information and makes decisions on broadening the information seeking process beyond local resources</td>

<td valign="top">Demonstrate information competence and the ability to use computers and other technology for many purposes.</td>

</tr>

<tr>

<td valign="top">**4.** In considering the following article citation, what does _64_(20) represent?  

Kors, A. C. (1998). Morality on today's college campuses: The assault upon liberty and dignity. _Vital Speeches of the Day, 64_(20), 633-637.</td>

<td valign="top">Identify the parts of a citation and accurately craft bibliographical references.</td>

<td valign="top">Differentiate between the types of sources cited and understand the elements and correct syntax of a citation for a wide range of resources</td>

<td valign="top">Quote, paraphrase and cite correctly from a variety of media sources</td>

</tr>

<tr>

<td valign="top">**5.** In an online database which combination of keywords below would retrieve the greatest number of records?</td>

<td valign="top">Conduct database searches using Boolean strategy, controlled vocabulary and limit features</td>

<td valign="top">Construct and implement effectively-designed search strategies</td>

<td valign="top">Formulate a researchable topic that can be supported by database search strategies</td>

</tr>

<tr>

<td valign="top">**6.** If you find a very good article on your topic, what is the most efficient source for finding related articles?</td>

<td valign="top">Follow cited references to obtain additional relevant information</td>

<td valign="top">Compare new knowledge with prior knowledge to determine the value added</td>

<td valign="top">Locate and use relevant databases…and interpret results of research studies</td>

</tr>

<tr>

<td valign="top">**7.** What is an empirical study?</td>

<td valign="top">Distinguish among methods used in retrieved articles</td>

<td valign="top">Identify appropriate investigative methods</td>

<td valign="top">Explain different research methods used by psychologists</td>

</tr>

<tr>

<td valign="top">**8.** Which area of the SJLibrary.org web site provides a list of core databases for different student majors?</td>

<td valign="top">Identify core databases in the discipline</td>

<td valign="top">Select the most appropriate investigative methods or information retrieval systems for accessing the needed information</td>

<td valign="top">Locate and choose relevant sources from appropriate media</td>

</tr>

<tr>

<td valign="top">**9.**What does the following citation represent:  

Erzen, J. N. (2007). Islamic aesthetics: An alternative way to knowledge. _Aesthetics and Art Criticism, 65_ (1), 69-75.</td>

<td valign="top">Identify the parts of a citation and accurately craft bibliographical references.</td>

<td valign="top">Differentiate between the types of sources cited and understands the elements and correct syntax of a citation for a wide range of resources</td>

<td valign="top">Identify and evaluate the source, context and credibility of information</td>

</tr>

<tr>

<td valign="top">**10.** If you are searching for a book or article your library does not own, you can get a free copy through:</td>

<td valign="top">Determine local availability of cited item and use Link+ and Interlibrary Loan services as needed</td>

<td valign="top">Determine the availability of needed information and makes decisions on broadening the information seeking process beyond local resources</td>

<td valign="top">Locate and choose relevant sources from appropriate media</td>

</tr>

<tr>

<td valign="top">**11.** How would you locate the hard-copy material for this citation?  

Erzen, J. N. (2007). Islamic aesthetics: An alternative way to knowledge. _Aesthetics and Art Criticism, 65_ (1), 69-75.</td>

<td valign="top">Search library catalog and locate relevant items</td>

<td valign="top">Uses various search systems to retrieve information in a variety of formats</td>

<td valign="top">Locate and use relevant databases…and interpret results of research studies</td>

</tr>

</tbody>

</table>

As outlined in the literature, best practices were followed in developing the survey questions. Hansen and Dexter provide a valuable set of guidelines on how to write quality multiple-choice questions, noting they 'can be used to measure a range of learning outcomes and can provide a reliable assessment of a student's progress([Hansen and Dexter 1997](#han97): 1). Many of their guidelines are aligned with those that Allen and Babbie ([2008](#all08)) recommend including creating short, clear questions that are culturally sensitive, avoiding the use of negative words such as _not_ in question statements and steering clear of biased language that may lead participants to answer correctly or in a way that controls response outcomes.

The creation of multiple-choice questions was undertaken with an understanding of their limitations. Much has been written about the disadvantages of fixed-choice questions, including their testing of memory recall rather than higher order thinking skills ([Oakleaf 2008](#oak08)). They are further criticised for rewarding guessing ([Oakleaf 2008](#oak08)). Additionally, Carter ([2002](#car02)) discusses the benefits of Barclay's ([1993](#bar93)) method of using open-ended questions because they encourage students' natural thought process, a closer reflection of the real-world research process as opposed to the artificial test environment posed by multiple-choice questions ([Oakleaf 2008](#oak08)).

We decided on multiple-choice questions for this study to allow for immediate computation of results. Past experience at the San Jose State University Library has shown that student commentary is time-consuming to code and standardise. Given the recent California State University budget cuts resulting from state-wide deficits, fixed-choices offer a practical and inexpensive alternative to more resource-intensive approaches to data analysis for organisation-wide assessment.

To address some of the drawbacks of quantitative data collection, each multiple-choice question had four distracters (i.e., incorrect responses). This increased the level of thought necessary to eliminate perceived wrong answers ([Jensen _et al._ 2006](#jen06)). Survey questions also had an option labelled _not sure_ to minimize guessing ([Radcliff _et al._ 2007](#rad07)). This wording was used to lessen the stigma associated with a _don't know_ response. This option also guaranteed that every question had an obvious relatable answer, particularly if none of the other choices seemed correct ([Radcliff _et al._ 2007](#rad07)). To further encourage students to choose the _not sure_ option if they did not know the correct answer, scripted instructions before the pre-test emphasized that scores were not being graded or shared with others and that answering truthfully would help librarians improve instructional services.

Many students chose the _not sure_ option in this study, particularly on the pre-test, illustrating this was an effective strategy in reducing student guessing. Future studies in which students are surveyed on their guessing behaviour would need to be conducted to explore this in more depth. See [Appendix D](#appD) for more details.

### Standardisation

All survey questions were reviewed to ensure they met current standards of quality assurance. Creating an effective survey instrument involved the evaluation of multiple-choice questions by individual undergraduate students, faculty members from the Psychology Department and experts at the San Jose State University Office of Institutional Research.

Questions and learning outcomes were first emailed to approximately fifty-five psychology faculty members for feedback. A few responded requesting that additional questions be added to the tool. These were incorporated into the survey instrument. Subsequently, questions were reviewed by experts at the Center for Assessment under the Office of Institutional Research. They provided valuable guidance on how to clarify wording and avoid standard pitfalls in developing survey questions. The authors found that the survey questions and their connection to various educational standards encouraged effective collaboration with campus offices and psychology faculty members to optimise student learning, a process that has been an ongoing challenge at the University.

After the initial review process by psychology faculty members and assessment experts, the principal author conducted cognitive interviews with students further to address the clarity of survey questions. Five undergraduate students representing different ethnic backgrounds participated in the one-on-one cognitive interviews for this study. While the principal author would have liked to interview more students, the interview sessions were time-consuming and it was difficult to solicit volunteers for a one-hour time commitment. Nonetheless, Allen and Babbie note that '_the pretest sample can be small—10 people or less_' ([Allen and Babbie 2008](#all08): 211). During each interview session, a set of scripted instructions adapted from a tool developed by Willis ([2005](#wil05)) was read to an interviewee. Each student was asked to read the survey questions thinking aloud about their clarity. Additionally, two scripted probing questions were asked of students once they finished commenting on each survey question:

*   In your own words, what is this question asking?
*   Are there any terms that are unclear to you?

See [Appendix B](#appB) for more details regarding the script.

This study involved a hybrid method of cognitive interviewing in pre-testing survey questions ([Beatty and Willis 2007](#bea07)). Rather than interviewing students with strictly scripted questions and no oral intervention (or on the other extreme, conducting inconsistent, probing interviews across interviewees) the hybrid method of cognitive interviewing allowed participants to think aloud about each survey question while the interviewer followed up with scripted questions asked consistently of all interviewees. This model had the advantage of gathering uninfluenced responses from participants with a systematic means of clarifying them.

Much has been written on the value of cognitive interviews in improving the validity and reliability of survey questionnaires. Desimone and Le Floch note '_[t]oo often we create inquiry tools without validating our measures against how respondents interpret our questions and therefore collect data of questionable quality_' ([Desimone and Le Floch 2004](#des04): 18). In one study, a combination of expert advice and qualitative methods, including cognitive interviews, was used to improve a nationally disseminated student survey ([Ouimet _et al._ 2004](#oui04)). In another study, Hughes ([2004](#hug04)) found that cognitive interviews assisted in revealing comprehension problems of survey questions.

Based on student feedback in this study, survey questions were further refined. Student interviewees had trouble with the term _currency_ in one of the survey questions. This word was used to indicate a current, or up-to-date source, whereas students interpreted it to mean one that would cost money. Hence the question was re-worded to clarify its intended meaning. See Table 1 for more details.

### Pilot study

Once the survey questions were evaluated for quality, a pilot study was conducted during the 2008 Winter Session with students in an upper division psychology course.

The purpose of the pilot study was to determine the average length of time to complete surveys, the overall impact of the surveys on the delivery of content in the instruction session, whether there were any technical problems with the survey interface such as login and usability problems and whether survey questions continued to be clear and comprehensible to students.

The pilot study revealed that survey administration took a total of ten minutes on average (five minutes for the pre-test and five minutes for the post-test) and did not negatively impact the coverage of content during the information literacy session. One problem arose when students taking the post-test entered their login incorrectly or with typing errors. However, this was easily remedied by having them re-enter the login with deliberate care.

None of the students raised questions about the wording or clarity of the survey instrument during the pilot test. However, the principal author did not solicit this input from them, mainly because the short time-frame of the instruction session was a concern. Nevertheless, it was encouraging that no confusion was raised, despite the possibility of remaining ambiguities.

### The study

#### Study population

Approval was obtained from the Institutional Review Board on campus and student volunteers were solicited from five upper division psychology courses to participate in this assessment study. The upper division psychology course was targeted in particular because it fulfils a strong writing and research component of the curriculum. Psychology faculty members teaching five of the nine sections offered during the Spring 2009 semester requested library instruction sessions and consented to student participation. Psychology courses were specifically identified for this study because the principal author served as the Psychology Librarian at that time. Consequently, coordination of library instruction sessions for these courses was convenient and easy to facilitate.

Since the core assignments, learning outcomes, survey questions and library instruction treatments were the same, the authors combined student results for all five sections. Students who participated in the pre- and post-tests during a regularly scheduled library instruction session comprised the entire survey population (n = 83). Most were junior-level undergraduates (69.9%) with a smaller percentage of senior-level undergraduates (30.1%).

The junior-level psychology courses are typically geared towards those majoring in the department, which accounts for the high percentage of psychology majors (92.8%). Other majors included Social Work (2.4%), Communicative Disorders and Sciences (2.4%), Economics (1.2%) and Justice Studies (1.2%).

Further demographic information is included in [Appendix A](#appA) and will be used for more in-depth analysis in future studies.

#### Implementation

The study was conducted in the Spring 2009 semester. The principal author conducted all psychology instruction sessions because she was the only librarian dedicated to the subject of psychology at the time of the study.

As with the pilot study, the locally-designed, assessment application was used to create a survey containing expertly-evaluated multiple-choice questions matching social sciences learning outcomes. The principal author coordinated with psychology faculty members in selecting questions relevant to their course goals and stressed that students be prompt to ensure adequate time for assessment and instruction.

Once the appropriate survey questions were chosen, the assessment tool automated the development of the customised survey instrument. The newly developed survey was then created as a link on the Library's psychology research guide, an online resource used as a starting point for the instruction sessions. After each session, the assessment tool provided immediate access to student scores in an HTML, Excel, or Word format for review.

Before each instruction session, the principal author set up all computer stations in the classroom to show the direct link to the pre-test survey. This was done in an effort to save time once the students arrived. All five psychology library instruction sessions took place in the same computer laboratory to ensure environmental consistency. Course outlines and handouts were disseminated to students after they completed the post-test at the end of the instruction session. This was to encourage reliance upon their own incoming knowledge and subsequent understanding of information literacy concepts.

Before instruction, the principal author read a script (see [Appendix C](#appC)) explaining the purpose of the study, emphasizing anonymity, confidentiality, voluntary participation and the benefits to the programme of a better understanding of student learning needs. Voluntary participation was an intentional part of the research design and is supported as an effective means of gathering accurate data. Portmann and Roush ([2004](#por04)) conducted pre- and post-test surveys during a library instruction intervention and found that students do not provide thoughtful responses when motivated by other means, such as receiving extra credit.

The principal author was careful to cover all learning objectives during the instruction session without referring to content in the direct wording of the multiple-choice survey questions. This would have encouraged memory recall rather than critical thinking on the post-test. Each instruction session lasted seventy-five minutes. This allowed for about an hour of research orientation and active learning exercises before the post-test.

Participants accessed the pre-test survey through their student identity number. They answered six background questions, then 11 multiple-choice questions covering eight learning objectives related to their course. After the instructional intervention, participants used the same student number to access the post-test. Using the student number ensured a unique login for each participant. This login also served to link individual student pre-test scores with corresponding post-test scores.

The post-test contained the same multiple-choice survey questions as the pre-test to provide a direct comparison of scores in assessing the impact of library instruction on student learning. However, the post-test did not repeat the background questions contained on the pre-test as these data had already been collected and would not change.

While a post-test given directly after a library instruction session may lead to reliance on memory recall, every effort was made to avoid this problem as well as other limitations associated with multiple-choice tests (see [Question development](#qd) above for more details). Nevertheless, in a future study, the authors plan to investigate whether students retain information provided in library instruction sessions over a longer period of time. The longitudinal study would involve administering the pre-test to a class at the beginning of a semester and follow up by administering the post-test to the same class at the end of the semester. Collecting data from the same set of students ensures a direct comparison of the before and after data.

In this study, ninety-four students initially participated in the assessment study. However, students who completed the pre-test survey but failed to take the post-test survey were automatically eliminated from the data pool, which resulted in eighty-three valid student participant data sets. Unfortunately, we were unable to ascertain why some students did not complete the post-test. The Institutional Review Board mandates student anonymity and as a result, individual identities were not available to the authors. Regardless, the Board guidelines also stipulate that student participation be voluntary. Consequently, even if the authors were privy to personal profiles, it was unethical to solicit information about students' lack of participation.

## Results and data analysis

### Results and analysis of specific survey questions

The authors provide a bird's-eye view of pre- and post-test scores, laying the groundwork for future research projects and general directions in library instruction.

One overall finding was that student scores improved on every survey question after the students received an instructional intervention. Of particular note, the entire student population (n = 83) scored high relating to questions about scholarly and popular sources on the post-test survey. Every student scored correctly on Question 1 of the post-test:

*   Imagine you have an assignment to write a paper based on scholarly information. Which would be the most appropriate source to use?

Moreover, most (91.6%) scored correctly on Question 2 of the post-test:

*   How can you tell you are reading a popular magazine?

However, pre-test scores for Question 1 and Question 2 showed a gap in student score ranges. While 92.8% of participants responded correctly to Question 1 in the pre-test, only 69.9% provided correct answers for Question 2 on this same topic. This suggests that multiple survey questions covering different aspects of an information literacy concept are needed to test student comprehension thoroughly.

While students scored well on questions related to scholarly and popular literature, they had the most difficulty identifying the Web page containing core databases for different student majors (Question 8). On the pre-test, only 50.6% answered this question correctly. A possible reason for this may be that the Web page, at the time, was labelled _SJSU Research Topics_. Such terminology may have caused confusion as _research topics_ does not adequately translate to _core databases for different student majors_. This interpretation is also supported by the post-test results for Question 8\. Only 59.0%, the lowest post-test score, of the participants answered this question correctly, despite repeated exposure to the SJSU Research Topics Web page during the library instruction session. Given the emphasis placed on this Web page for identifying subject-specific resources, the difference in pre- and post-test student scores (8.4%) indicates that, at the least, students answered Question 8 without much reliance on memory recall.

Before the library instruction session, students did fairly well in identifying parts of a journal citation (84.3% answered Question 4 correctly). However, there was confusion over what type of source a particular citation represented (67.5% answered Question 9 correctly). Additional pre-test difficulties arose in understanding Boolean logic (39.8% answered Question 5 correctly), the value of article bibliographies in discovering other related sources for research assignments (47.0% answered Question 6 correctly), how to obtain material the library does not own from other services (49.4% answered Question 10 correctly) and, of particular note, how to locate a hard-copy version of an article (33.7% answered Question 11 correctly). See Table 2 for more details.

Through discourse with students in the classroom, in office hour consultations and at the reference desk, the most popular research request is for immediate access to full-text articles. Given this attention to instantaneous document retrieval, a logical shortfall is that students are probably less skilled at negotiating the print journal collection. As the survey pre-test scores show, many students are unaware of how to look up a journal title in the library catalogue, get a call number and check the physical volumes to get an article that is unavailable online (Question 11). Similarly, students are unfamiliar with how to acquire materials unavailable at the San Jose State University Library, perhaps, again, because this requires additional time. This may account for why only 49.4% of respondents answered Question 10 correctly on the pre-test.

However, students showed the greatest improvement in post-test scores for Question 10 and Question 11\. Their awareness of interlibrary lending services increased by 36.1% on the post-test (85.5% answered correctly). Similarly, students' understanding of how to locate print journals improved by 36.2% on the post-test (69.9% answered correctly), although the post-test scores on this topic still seem poor.

Student post-test scores for Question 5 on Boolean logic and Question 6 on the importance of bibliographies also raise questions about the significance of results. On Question 5, student scores improved by 25.2% on the post-test (65.0% answered correctly) and on Question 6, there was a 25.3% increase in post-test scores (72.3% answered correctly). However, is the seventieth percentile an acceptable marker for correct answers? It seems frustrating that, if this were so, roughly twenty-three of the respondents would still have trouble understanding a basic research concept.

Determining an acceptable score range for Question 7 (75.9% answered correctly on the post-test), which tests student comprehension of different research methodologies, is even more problematic. One could argue that an acceptable score for this survey question should be higher than the norm since junior-level psychology students are often required to find empirical studies in supporting their research. They may be able to find relevant articles without understanding and conducting Boolean searches (Question 5), but they would not be able to fulfil their assignments without understanding the definition of an empirical study.

While this exploratory study provides general points for discussion of student learning needs, future research and an analysis of student's incoming experience with library instruction will allow the authors to comment more thoroughly on the significance of student test scores.

<table width="100%" border="1" cellspacing="0" cellpadding="6" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd"><caption align="bottom">  
**Table 2\. Participant difference scores after library instruction intervention  
***Percentage and number of students who answered the question correctly.</caption>

<tbody>

<tr>

<th>Question</th>

<th>Pre-test Score*</th>

<th>Post-test Score*</th>

<th>Difference</th>

</tr>

<tr>

<td>**1.** Imagine you have an assignment to write a paper based on scholarly information. Which would be the most appropriate source to use?</td>

<td align="center">92.8% (77)</td>

<td align="center">100.0% (83)</td>

<td align="center">+7.2%</td>

</tr>

<tr>

<td>**2.** How can you tell you are reading a popular magazine?</td>

<td align="center">69.9% (58)</td>

<td align="center">91.6% (76))</td>

<td align="center">+21.7%</td>

</tr>

<tr>

<td>**3.** What is the name of the linking tool found in SJSU databases that may lead you to the full text of an article?</td>

<td align="center">73.4% (61)</td>

<td align="center">88.0% (73)</td>

<td align="center">+14.6%</td>

</tr>

<tr>

<td>**4.** In considering the following article citation, what does _64_(20) represent?  

Kors, A. C. (1998). Morality on today's college campuses: The assault upon liberty and dignity. _Vital Speeches of the Day, 64_(20), 633-637.</td>

<td align="center">84.3% (70)</td>

<td align="center">91.6% (76)</td>

<td align="center">+7.3%</td>

</tr>

<tr>

<td>**5.** In an online database which combination of keywords below would retrieve the greatest number of records?</td>

<td align="center">39.8% (33)</td>

<td align="center">65.0% (54)</td>

<td align="center">+25.2%</td>

</tr>

<tr>

<td>**6.** If you find a very good article on your topic, what is the most efficient source for finding related articles?</td>

<td align="center">47.0% (39)</td>

<td align="center">72.3% (60)</td>

<td align="center">+25.3%</td>

</tr>

<tr>

<td>**7.** What is an empirical study?</td>

<td align="center">57.8% (48)</td>

<td align="center">75.9% (63)</td>

<td align="center">+18.1%</td>

</tr>

<tr>

<td>**8.** Which area of the SJLibrary.org web site provides a list of core databases for different student majors?</td>

<td align="center">50.6% (42)</td>

<td align="center">59.0% (49)</td>

<td align="center">+8.4%</td>

</tr>

<tr>

<td>**9.**What does the following citation represent:  

Erzen, J. N. (2007). Islamic aesthetics: An alternative way to knowledge. _Aesthetics and Art Criticism, 65_ (1), 69-75.</td>

<td align="center">67.5% (56)</td>

<td align="center">83.1% (69)</td>

<td align="center">+15.6%</td>

</tr>

<tr>

<td>**10.** If you are searching for a book or article your library does not own, you can get a free copy through:</td>

<td align="center">49.4% (41)</td>

<td align="center">85.5% (71)</td>

<td align="center">+36.1%</td>

</tr>

<tr>

<td>**11.** How would you locate the hard-copy material for this citation?  

Erzen, J. N. (2007). Islamic aesthetics: An alternative way to knowledge. _Aesthetics and Art Criticism, 65_ (1), 69-75.</td>

<td align="center">33.7% (28)</td>

<td align="center">69.9% (58)</td>

<td align="center">+36.2%</td>

</tr>

</tbody>

</table>

### Discussion and future investigation

In addition to exploring an institution-specific model for information literacy assessment, another purpose of this study was to determine the feasibility of administering pre- and post-test surveys during the short period of time in which library instruction is provided. This is critical to optimising student learning opportunities and convincing other librarians to use the tool during their instruction sessions. Findings from this study demonstrate a feasible means of assessing student information literacy competence in the classroom setting, given close coordination with campus faculty.

### Programme improvements

Nevertheless, the condensed time-frame of information literacy instruction sessions merits further attention, particularly since embedded assessment requires vigilant organisation and limits what can be measured.

This study explored the assessment of eight student learning outcomes, permitting only a short period of time to address each one during an hour allotted for instruction. Such broad content coverage can overwhelm students and reduce the reliability of assessment efforts ([Baume 2001](#bau01)). Consequently, the large number of learning outcomes tested in the study may account for why students performed better on some questions than on others. Moreover, eleven multiple-choice questions seem inadequate to sufficiently test students on eight different research skills. Ideally, two to four questions should be developed for each learning outcome ([Persky and Pollack 2008](#per08)). Unfortunately, the authors were unable to lengthen the survey because of time constraints. However, recent attention to electronic teaching aids offers promising support for maximising student learning through more focused library instruction content.

For the past several years, the California State University has assembled a core set of digital learning objects to support system-wide information literacy instruction programmes ([California State University 2009](#csu09)). Digital learning objects, by definition, are a derivative of the object-oriented programming concept in that they are succinct, self-contained pieces of instructional media ([Hunsaker _et al._ 2009](#hun09)) covering individual research skills. They are easily reusable ([Clyde 2004](#cly04)) and do not require students to take an hour-long, comprehensive tutorial to gain proficiency in one specific research activity.

Currently, San Jose State University librarians have developed a number of digital learning objects covering content including scholarly and popular research material, the use of call numbers and the use of a popular database (Academic Search Premier). The authors recommend the continued development of digital learning objects that educate students on basic skills such as the identification of particular sources, learning the classification system, using Boolean search operators, refining a research topic and understanding plagiarism. The California State University digital learning objects core team has identified similar types of content in digital form that address many of these information literacy concepts, interactive tools that should be referenced from the library Web site and not reinvented locally ([California State University 2009](#csu09)).

More and more courses and programmes are only available in an online format at San Jose State University. In this electronic learning environment, students are increasingly expected to investigate and familiarise themselves with web-based research tools on their own to achieve academic success. A thoughtful approach to building a digital learning object library that supports and tests student learning of particular research skills would greatly enhance the information literacy programme. Currently, students may or may not stumble across digital learning objects available on the library Web site. There is no infrastructure that ties digital learning objects to specific programmes on campus. Ideally, librarians would select digital learning objects appropriate for particular disciplines and courses that could be embedded within campus curricula and required for students to complete as part of their academic programme. This would allow librarians to focus on a small set of course-specific learning outcomes during instruction sessions rather than trying to cover every aspect of the research process in a short timeframe.

In addressing fewer learning outcomes, librarian instructors gain the added benefit of assigning more questions for each research concept being tested. For example, instead of selecting eleven survey questions to test eight learning outcomes, a library instructor could use the same number of questions to test three learning outcomes, thus providing a more reliable assessment of student learning.

Based on findings from this study and characteristics of quality digital learning objects, the authors have considered how to modify future assessment surveys for a more efficient and comprehensive examination of student learning.

A desirable feature of digital learning objects is _re-usability_ ([Watson 2010](#wat10)), which refers to learning objects that are standalone training modules, those that are not tied to institutions through branding and those that can be easily adapted, or re-purposed, by others ([Watson 2010](#wat10)). This reduces the amount of time and money spent creating training materials that support library instruction programmes. Digital learning object re-usability can be taken a step further to mean the use of material that is unchanging, content that can be _re-used_ indefinitely without having to be updated. For example, time-tested content such as Boolean logic, citation analysis and the difference between scholarly and popular literature lends itself better to the digital learning object format than learning how to use a popular database, the interface of which will change from time to time.

In reviewing the information literacy concepts covered by the current assessment survey, the authors would probably retain Question 3 on the linking tool found in San Jose State University databases and Question 8 on the area of the library Web site that lists databases by major, relying on digital learning objects to cover the remaining topics. More questions (two to four for each learning objective) testing students' knowledge of subject-specific resources could be added to the survey and form the basis of the library instruction session. Such an approach would permit librarians to focus on content that is more complex and evolving, while topics of a static nature could be addressed by digital learning objects.

### Future studies

In analysing student pre- and post-test scores, the authors have targeted preliminary ways of improving instructional efficacy and information literacy skills. Additionally, they recommend analysing a wider range of data in future assessment studies to reflect a solid, scientific approach.

To this end, the authors continue to coordinate with colleagues at the San Jose State University's Office of Institutional Research in reviewing new survey questions for future studies. Through this dialogue, assessment specialists propose that a stronger basis for analysis would enhance and enrich assessment findings. Consequently, the principal author has applied for and been granted monies to hire a statistician. The statistician will build a mathematical template incorporating student background variables, their relationship with each other as a whole and their collective impact on assessment results. As data is collected through online surveys, the template will automate statistical computations and reports, yielding a more complete picture of student research skills and instructional efficacy.

Additionally, for future studies, the authors plan to:

*   conduct more cognitive interviews to strengthen the quality of survey questions,
*   customise survey questions for particular subject areas whenever possible,
*   include background questions that concentrate on comfort levels with computer applications and past experience with various library tutorials,
*   determine what is considered an acceptable student score range in survey responses,
*   investigate how the assessment tool can be used for longitudinal analysis of research skills.

## Conclusion

With the aid of a discipline-based assessment tool containing standardised, multiple-choice questions, the authors have been provided a means of administering pre- and post-test surveys to evaluate student learning during library instruction sessions. The practicality of the tool and its impact on time management was tested with five junior-level psychology courses during the Spring 2009 semester. Preliminary data about student comprehension of information literacy concepts was also gathered.

While scores improved for every survey question, some concepts were more difficult than others for student participants to understand. Student comprehension was likely to have been affected by the condensed time-frame of instruction sessions and the limited ability of the library instructor to provide in-depth coverage of individual learning objectives. Given the inconsistency among students' post-test scores across survey questions, the authors recommend that library instructors focus on a few key concepts. This will maximise instruction time and students' ability to reliably comprehend what is taught. The strategic development of digital learning objects that address basic learning goals will make it possible for instructors to concentrate on a few course-specific skills such as navigating core subject databases. Future studies, however, will provide additional data analysis and reveal a more complete picture of student learning needs.

The authors hope the use of this survey application and a more thorough approach to statistical analysis will encourage a unified, organisation-wide approach to information literacy assessment at the San Jose State University Library. The ability to illustrate effective modes of teaching and programmatic improvement will not only enhance student learning, but strengthen librarian ties with campus faculty, bolster documentation during the accreditation process, provide publication opportunities for individual library faculty and increase possibilities for campus funding support of library services.

### Acknowledgements

We wish to thank the following colleagues for their willingness to proofread our document and/or offer critical commentary: Dr. Berkeley Miller, Dr. Sutee Sujitparapitaya, Dr. Lili Luo, Dr. Mary Somerville, Christina Peterson (MLIS), Bridget Kowalczyk (MLIS), Diana Wu (MLIS), Francis Howard (MLIS) and Robert Bruce (MLIS).

Thanks to Rebecca Feind (MLIS) and Lydia Collins (MLIS) for providing feedback during the design of the online assessment tool. Also thanks to Lyna Nguyen and Jessie Cai for developing the computer programming for the tool.

## About the authors

**Shannon M. Staley**, the corresponding author, is a library faculty member at San Jose State University. Formerly the Psychology Librarian, she now serves as the Education Librarian. She provides instruction, collection development and research services for departments in the College of Education. She can be contacted at [Shannon.Staley@sjsu.edu](mailto:Shannon.Staley@sjsu.edu).  

**Nicole A. Branch** and **Tom L. Hewitt** are graduate students at the School of Library and Information Science at San Jose State University. Nicole can be contacted at [nicolebranch13@gmail.com](mailto:nicolebranch13@gmail.com). Tom can be contacted at [tom_l_hewitt@msn.com](mailto:tom_l_hewitt@msn.com).

## References

*   Allen, R. & Babbie, E. R. (2008). _Research methods for social work_. Belmont, CA: Thomson Brooks/Cole.
*   American Library Association. (2003). [_Characteristics of programs of information literacy that illustrate best practices: a guideline_](http://www.webcitation.org/5olrItLwn). Chicago, IL: American Library Association. Retrieved June 19, 2009 from http://www.ala.org/ala/mgrps/divs/acrl/standards/characteristics.cfm (Archived by WebCite® at http://www.webcitation.org/5olrItLwn)
*   American Library Association. (2006a). [_Guidelines for instruction programs in academic libraries_](http://www.webcitation.org/5oncuELZf). Chicago, IL: American Library Association. Retrieved July 14, 2009 from http://www.ala.org/ala/mgrps/divs/acrl/standards/guidelinesinstruction.cfm (Archived by WebCite® at http://www.webcitation.org/5oncuELZf)
*   American Library Association. (2006b). [_Information literacy competency standards for higher education_](http://www.webcitation.org/5ond2Mtr6). Chicago, IL: American Library Association. Retrieved July 7, 2009 from http://www.ala.org/ala/mgrps/divs/acrl/standards/informationliteracycompetency.cfm (Archived by WebCite® at http://www.webcitation.org/5ond2Mtr6)
*   American Psychological Association. (2007). [_APA guidelines for the undergraduate psychology major_](http://www.webcitation.org/5ond96duc). Washington, DC: American Psychological Association. Retrieved January 26, 2010 from http://www.apa.org/ed/precollege/about/psymajor-guidelines.pdf (Archived by WebCite® at http://www.webcitation.org/5ond96duc)
*   Barclay, D. (1993). Evaluating library instruction: doing the best you can with what you have. _Reference Quarterly_, **33**(2), 195-202.
*   Beatty, P. C. & Willis, G. B. (2007). Research synthesis: the practice of cognitive interviewing. _Public Opinion Quarterly_, **71**(2), 1-25.
*   Blixrud, J. C. (2003). Project SAILS: standardized assessment of information literacy skills. _ARL Bimonthly Report_, (230/231), 18-19.
*   Bogel, G. (2008). Facets of practice. _Knowledge Quest_, **37**(2), 10-15\.
*   <a name="bro95" id="bro95"></a>Brookfield, S. (1995). _Becoming a critically reflective teacher_. San Francisco, CA: Jossey-Bass.
*   Baume, D. (2001). _[A briefing on assessment of portfolios.](http://www.webcitation.org/5s9NOUjOq)_ York, UK: Learning & Teaching Support Network. (LTSN Generic Centre Assessment Series No.6). Retrieved 20 August, 2010 from http://www.palatine.ac.uk/files/973.pdf (Archived by WebCite® at http://www.webcitation.org/5s9NOUjOq)
*   California State University. (2009). [_Digital learning objects_](http://www.webcitation.org/5ondEtg6l). Rohnert Park, CA: California State University. Retrieved January 1, 2010 from http://infoguides.sdsu.edu/ict/all.php (Archived by WebCite® at http://www.webcitation.org/5ondEtg6l)
*   California State University. (2007). [_ICT literacy in the CSU: digital learning objects database_](http://www.webcitation.org/5ondN5rhB). Rohnert Park, CA: California State University. Retrieved January 1, 2010 from http://teachingcommons.cdl.edu/ictliteracy/dlo/ (Archived by WebCite® at http://www.webcitation.org/5ondN5rhB)
*   California State University. (n.d.). [_CSU Information/ICT literacy strategic directions_](http://www.webcitation.org/5ondYdZqp). Rohnert Park, CA: California State University. Retrieved July 7, 2009 from http://teachingcommons.cdl.edu/ictliteracy/about/documents/CSUILStrategicDirections_SB6_17_07rev.pdf (Archived by WebCite® at http://www.webcitation.org/5ondYdZqp)
*   California State University. (2007). [_Information & communication technology literacy in the CSU_](http://www.webcitation.org/5ondfeIKX). Rohnert Park, CA: California State University. Retrieved June 9, 2009 from http://teachingcommons.cdl.edu/ictliteracy/assessment/index.html (Archived by WebCite® at http://www.webcitation.org/5ondfeIKX)
*   Cameron, L., Wise, S.L. & Lottridge, S.M. (2007). The development and validation of the information literacy test. _College & Research Libraries_, **68**(3), 229-236.
*   Carter, E.W. (2002). 'Doing the best you can with what you have:' lessons learned from outcomes assessment. _Journal of Academic Librarianship_, **28**(1), 36-41.
*   Clyde, L. A. (2004). Digital learning objects. _Teacher Librarian_, **31**(4), 55-57.
*   Desimone, L. M. & Le Floch, K. C. (2004). Are we asking the right questions? Using cognitive interviews to improve surveys in education research. _Educational Evaluation and Policy Analysis_, **26**(1), 1-22.
*   Donovan, C. & Winterman, B. (2009). [_Throw away the map: blazing new trails between information literacy and the disciplines_](http://www.webcitation.org/5ondwxdxC). Electronic presentation retrieved April 5, 2010 from http://prezi.com/6jbzxecgbcum (Archived by WebCite® at http://www.webcitation.org/5ondwxdxC)
*   Educational Testing Service. (2009). [_ETS iSkills assessment_](http://www.webcitation.org/5ondwxdxC). Princeton, NJ: Educational Testing Service. Retrieved August 10, 2009 from http://www.ets.org/iskills (Archived by WebCite® at http://www.webcitation.org/5ondwxdxC)
*   Gilstrap, D.L. & Dupree, J. (2008). Assessing learning, critical reflection and quality educational outcomes: the critical incident questionnaire. _College & Research Libraries_, **69**(5), 407-426.
*   Hansen, J.D. & Dexter, L. (1997). Quality multiple-choice test questions: item-writing. _Journal of Education for Business_, **73**(2), 94-97.
*   Hughes, K. A. (2004). _Comparing pretesting methods: cognitive interviews, respondent debriefing and behavior coding_. Washington D.C.: U.S. Bureau of the Census.
*   Hunsaker, M., Howard, F., Liu, S.H. & Davis, J. (2009). Digital learning objects: a local response to the California State University system initiative. _New Library World_, **110**(3/4), 151-160.
*   Jackson, P. A. (2006). Plagiarism instruction online: assessing undergraduate students' ability to avoid plagiarism. _College & Research Libraries_, **67**(5), 418-428.
*   Jensen, M., Duranczyk, I., Staats, S., Moore, R., Hatch, J. & Somdahl, C. (2006). Using a reciprocal teaching strategy to create multiple-choice exam questions. _American Biology Teacher_, **68**(6), 67-71.
*   Kenney, A. J. (2006). The final hurdle? _School Library Journal_, **52**(3), 62-64.
*   Kent State University. (2008). [_Project SAILS_](http://www.webcitation.org/5one8lCwK). Kent, OH: Kent State University. Retrieved August 10, 2009 from https://www.projectsails.org/sails/overview.php?page=aboutSAILS (Archived by WebCite® at http://www.webcitation.org/5one8lCwK)
*   Knight, L.A. (2002). The role of assessment in library user education. _Reference Services Review_, **30**(1), 15-24.
*   Koehler, B. & Swanson, K. (1988). ESL students and bibliographic instruction: learning yet another language. _Research Strategies_, **6**(4), 148-160.
*   Mathson, S. M. & Lorenzen, M. G. (2008). We won't be fooled again: teaching critical thinking via evaluation of hoax and historical revisionist Web sites in a library credit course. _College & Undergraduate Libraries_, **15**(1), 211-230.
*   Merz, L.H. & Mark, B.L. (2002). _Assessment in college library instruction programs_. Chicago, IL: Association of College & Research Libraries.
*   Oakleaf, M. (2008). Dangers and opportunities: a conceptual map of information literacy assessment approaches. _Portal_, **8**(3), 233-253.
*   Ouimet, J.A., Bunnage, J.C., Carini, R.M., Kuh, G.D. & Kennedy, J. (2004). Using focus groups, expert advice and cognitive interviews to establish the validity of a college student survey. _Research in Higher Education_, **45**(3), 233-250.
*   Persky, A.M. & Pollack, G.M. (2008). Using answer-until-correct examinations to provide immediate feedback to students in a pharmacokinetics course. _American Journal of Pharmaceutical Education_, **72**(4), 1-7.
*   Portmann, C.A. & Roush, A.J. (2004). Assessing the effects of library instruction. _The Journal of Academic Librarianship_, **30**(6), 461-465.
*   Radcliff, C.J., Jensen, M.L., Salem, Jr., J.A., Burhanna, K. & Gedeon, J.A. (2007). _A practical guide to information literacy assessment for academic librarians_. Westport, CT: Libraries Unlimited.
*   Rockman, I.F. & Smith, G.W. (2005). Information and communication technology literacy: new assessments for higher education. _College & Research Libraries News_, **66**(8), 587-589.
*   San Jose State University. (2007). [_SJSU strategic planning: mission & shared values_](http://www.webcitation.org/5oneIJdsR). Retrieved April 5, 2010 from http://www.sjsu.edu/president/mission/ (Archived by WebCite® at http://www.webcitation.org/5oneIJdsR)
*   San Jose State University. _Academic Senate_. (2004). [_Sense of the senate resolution: infusion of information literacy into the SJSU curriculum_](http://www.webcitation.org/5oneO56qG). San Jose, CA: San Jose State University. Retrieved July 15, 2009 from http://www.sjsu.edu/senate/SS-S04-9.htm (Archived by WebCite® at http://www.webcitation.org/5oneO56qG)
*   San Jose State University. _Library_. (2007). [_San Jose State University library strategic directions_](http://www.webcitation.org/5oneTZnKM). San Jose, CA: San Jose State University. Retrieved July 15, 2009 from http://www.sjlibrary.org/about/sjsu/planning/directions.pdf (Archived by WebCite® at http://www.webcitation.org/5oneTZnKM)
*   Schloman, B. F. & Gedeon, J. A. (2007). Creating TRAILS: tool for real-time assessment of information literacy skills. _Knowledge Quest_, **35**(5), 44-47.
*   Somerville, M. M., Smith, G. W. & Macklin, A.S. (2008). The ETS iSkills TM assessment: a digital age tool. _Electronic Library_, **26**(2), 158-171.
*   University of Texas at Austin. (1998). [_Texas information literacy tool_](http://www.webcitation.org/5ps1VjSWK). Austin, TX: University of Texas at Austin. Retrieved 19 January, 2009 from http://tilt.lib.utsystem.edu/resources/index.html (Archived by WebCite® at http://www.webcitation.org/5ps1VjSWK)
*   University of Texas at El Paso. _Center for Effective Teaching and Learning_. (n.d.). [_Formative and summative evaluation_](http://www.webcitation.org/5ondoU9mW). Retrieved December 29, 2009 from http://sunconference.utep.edu/CETaL/resources/portfolios/form-sum.htm (Archived by WebCite® at http://www.webcitation.org/5ondoU9mW)
*   Watson, J. (2010). A case study: developing learning objects with an explicit learning design. _Electronic Journal of e-Learning_, **8**(1), 41-50.
*   Western Association of Schools and Colleges. (2008). [_2008 handbook of accreditation_](http://www.webcitation.org/5oneZ5bUp). Alameda, CA: Western Association of Schools and Colleges. Retrieved June 19, 2009 from http://www.wascsenior.org/findit/files/forms/Handbook_of_Accreditation_2008_with_hyperlinks.pdf (Archived by WebCite® at http://www.webcitation.org/5oneZ5bUp)
*   Williams, J.L. (2000). Creativity in assessment of library instruction. _Reference Services Review_, **28**(4), 323-334.
*   Willis, G.B. (2005). _Cognitive interviewing: a tool for improving questionnaire design_. Thousand Oaks, CA: Sage Publications.



## Appendix A

### Information literacy survey

<table width="100%" border="0" cellpadding="10" cellspacing="0">

<tbody>

<tr>

<td colspan="2">

Please answer every question below. All information you provide is confidential. This survey instrument and research project has been approved by Graduate Studies and will take about 5 minutes to complete. You can read more about your rights as a participant and who to contact with any questions.

</td>

</tr>

<tr>

<td>**1\. Please indicate your Academic Level in School:**  
<input type="radio" name="academic_require" value="4" title="Freshmen - undergraduate">Freshmen - undergraduate  
<input type="radio" name="academic_require" value="41" title="Sophormore - undergraduate">Sophormore - undergraduate  
<input type="radio" name="academic_require" value="42" title="Junior - undergraduate">Junior - undergraduate  
<input type="radio" name="academic_require" value="43" title="Senior - undergraduate">Senior - undergraduate  
<input type="radio" name="academic_require" value="44" title="Graduate Student in Library &amp; Information Science">Graduate Student in Library & Information Science  
<input type="radio" name="academic_require" value="45" title="Graduate Student in another program">Graduate Student in another program  
<input type="radio" name="academic_require" value="46" title="Unclassified">Unclassified  
<input type="radio" name="academic_require" value="47" title="Other">Other  
</td>

</tr>

<tr>

<td>**2\. Please indicate your Gender:**  
<input type="radio" name="gender_require" id="gender1" value="1" title="Female">Female <input type="radio" name="gender_require" id="gender2" value="2" title="Male">Male</td>

</tr>

<tr>

<td>**3\. Please indicate your age:**  
<input type="radio" name="age_require" id="age1" value="1" title="Lesse than 18">Lesse than 18 <input type="radio" name="age_require" id="age2" value="2" title="18 to 29">18 to 29 <input type="radio" name="age_require" id="age3" value="3" title="30 to 39">30 to 39 <input type="radio" name="age_require" id="age4" value="4" title="40 to 49">40 to 49 <input type="radio" name="age_require" id="age5" value="5" title="50 or older">50 or older</td>

</tr>

<tr>

<td>**4\. Please specify your major:**  
<select name="major_require" title="major"><option value="">Choose one</option> <option value="1" title="Accounting &amp; Finance">Accounting & Finance</option> <option value="213" title="Administration of Justice">Administration of Justice</option> <option value="137" title="Aerospace Engineering">Aerospace Engineering</option> <option value="4" title="African American Studies">African American Studies</option> <option value="79" title="American Studies">American Studies</option> <option value="5" title="Anthropology">Anthropology</option> <option value="6" title="Art &amp; Design">Art & Design</option> <option value="7" title="Asian American Studies">Asian American Studies</option> <option value="103" title="Aviation">Aviation</option> <option value="10" title="Biological Sciences">Biological Sciences</option> <option value="151" title="Business">Business</option> <option value="11" title="Chemical &amp; Materials Engineering">Chemical & Materials Engineering</option> <option value="142" title="Chemistry">Chemistry</option> <option value="117" title="Child &amp; Adolescent Development">Child & Adolescent Development</option> <option value="115" title="Civil Engineering">Civil Engineering</option> <option value="16" title="Communication Studies">Communication Studies</option> <option value="15" title="Communicative Disorders &amp; Sciences">Communicative Disorders & Sciences</option> <option value="17" title="Computer Engineering">Computer Engineering</option> <option value="18" title="Computer Science">Computer Science</option> <option value="19" title="Counselor Education">Counselor Education</option> <option value="218" title="Creative Arts">Creative Arts</option> <option value="20" title="Economics">Economics</option> <option value="228" title="Education">Education</option> <option value="21" title="Education Leadership">Education Leadership</option> <option value="102" title="Electrical Engineering">Electrical Engineering</option> <option value="22" title="Elementary Education">Elementary Education</option> <option value="100" title="Engineering (General)">Engineering (General)</option> <option value="25" title="English &amp; Comparative Literature">English & Comparative Literature</option> <option value="136" title="Environmental Engineering">Environmental Engineering</option> <option value="26" title="Environmental Studies">Environmental Studies</option> <option value="27" title="Foreign Languages">Foreign Languages</option> <option value="28" title="Geography">Geography</option> <option value="29" title="Geology">Geology</option> <option value="32" title="Gerontology">Gerontology</option> <option value="212" title="Global Studies">Global Studies</option> <option value="33" title="Health Science">Health Science</option> <option value="36" title="History">History</option> <option value="233" title="Hospitality, Recreation &amp; Tourism Management">Hospitality, Recreation & Tourism Management</option> <option value="39" title="Humanities">Humanities</option> <option value="101" title="Industrial and Systems Engineering">Industrial and Systems Engineering</option> <option value="42" title="Instructional Technology">Instructional Technology</option> <option value="110" title="International Business">International Business</option> <option value="132" title="Jewish Studies">Jewish Studies</option> <option value="43" title="Journalism &amp; Mass Communications">Journalism & Mass Communications</option> <option value="75" title="Justice Studies">Justice Studies</option> <option value="148" title="Kinesiology">Kinesiology</option> <option value="44" title="Library &amp; Information Science">Library & Information Science</option> <option value="141" title="Linguistics">Linguistics</option> <option value="113" title="Management Information Systems">Management Information Systems</option> <option value="46" title="Marketing &amp; Decision Sciences">Marketing & Decision Sciences</option> <option value="152" title="Materials &amp; Chemical Engineering">Materials & Chemical Engineering</option> <option value="47" title="Mathematics">Mathematics</option> <option value="48" title="Mechanical Engineering">Mechanical Engineering</option> <option value="49" title="Meteorology">Meteorology</option> <option value="149" title="Mexican American Studies">Mexican American Studies</option> <option value="77" title="Middle East Studies">Middle East Studies</option> <option value="53" title="Music &amp; Dance">Music & Dance</option> <option value="34" title="Nursing">Nursing</option> <option value="54" title="Nutrition, Food Science and Packaging">Nutrition, Food Science and Packaging</option> <option value="35" title="Occupational Therapy">Occupational Therapy</option> <option value="56" title="Organization &amp; Management">Organization & Management</option> <option value="57" title="Philosophy">Philosophy</option> <option value="58" title="Physics">Physics</option> <option value="59" title="Political Science">Political Science</option> <option value="60" title="Psychology">Psychology</option> <option value="78" title="Religious Studies">Religious Studies</option> <option value="64" title="Secondary Education">Secondary Education</option> <option value="106" title="Social Science">Social Science</option> <option value="70" title="Social Work">Social Work</option> <option value="71" title="Sociology">Sociology</option> <option value="72" title="Special Education">Special Education</option> <option value="112" title="Taxation">Taxation</option> <option value="73" title="Technology">Technology</option> <option value="61" title="TV, Radio, Film &amp; Theatre">TV, Radio, Film & Theatre</option> <option value="74" title="Urban &amp; Regional Planning">Urban & Regional Planning</option> <option value="104" title="Women's Studies">Women's Studies</option></select></td>

</tr>

<tr>

<td>If you have a double major or your major is not in the dropdown above, please indicate other(s) below:  

1.<input type="text" name="doublemajor1" id="doublemajor1" size="50" maxlength="50" title="doublemajor 1">  

2.<input type="text" name="doublemajor2" id="doublemajor2" size="50" maxlength="50" title="doublemajor 2">  

3.<input type="text" name="doublemajor3" id="doublemajor3" size="50" maxlength="50" title="doublemajor 3"></td>

</tr>

<tr>

<td>**5\. Did you begin college at San Jose State University or elsewhere:**  
<input type="radio" name="college_require" id="college1" value="1" title="Started at SJSU">Started at SJSU <input type="radio" name="college_require" id="college2" value="2" title="Started elsewhere">Started elsewhere</td>

</tr>

<tr>

<td>**6\. Have you received library instruction before at SJSU Library:**  
<input type="radio" name="libraryinstr_require" id="libraryinstr1" value="1" title="Yes">Yes <input type="radio" name="libraryinstr_require" id="libraryinstr2" value="0" title="No">No <input type="radio" name="libraryinstr_require" id="libraryinstr3" value="2" title="Not Sure">Not Sure</td>

</tr>

<tr>

<td>**7\. In a typical 7-day week, about how many hours do you spend (in person or electronically) conducting research at King library:**  
<input type="radio" name="hours_require" id="hours1" value="1" title="None">None <input type="radio" name="hours_require" id="hours2" value="2" title="1-4">1-4 hours <input type="radio" name="hours_require" id="hours3" value="3" title="5-10">5-10 hours <input type="radio" name="hours_require" id="hours4" value="4" title="11-20">11-20 hours <input type="radio" name="hours_require" id="hours5" value="5" title="More than 20">More than 20 hours</td>

</tr>

<tr>

<td align="left">**1\. Imagine you have an assignment to write a paper based on scholarly information. Which would be the most appropriate source to use?**

*   <input type="radio" name="question-1_require" id="question-1" value="62-1" title="62-1">Magazine
*   <input type="radio" name="question-1_require" id="question-1" value="62-2" title="62-2">Journal
*   <input type="radio" name="question-1_require" id="question-1" value="62-3" title="62-3">Newspaper
*   <input type="radio" name="question-1_require" id="question-1" value="62-4" title="62-4">Web site
*   <input type="radio" name="question-1_require" id="question-1" value="62-5" title="62-5">Not sure

</td>

</tr>

<tr>

<td align="left">**2\. How can you tell you are reading a popular magazine?**

*   <input type="radio" name="question-2_require" id="question-2" value="63-1" title="63-1">There are few, if any, advertisements.
*   <input type="radio" name="question-2_require" id="question-2" value="63-2" title="63-2">Articles are in-depth and often have a bibliography.
*   <input type="radio" name="question-2_require" id="question-2" value="63-3" title="63-3">Articles are written for the general public.
*   <input type="radio" name="question-2_require" id="question-2" value="63-4" title="63-4">Issues are usually published quarterly (4 times a year)
*   <input type="radio" name="question-2_require" id="question-2" value="63-5" title="63-5">Not Sure

</td>

</tr>

<tr>

<td align="left">**3\. What is the name of the linking tool found in SJSU databases that may lead you to the full text of an article?**

*   <input type="radio" name="question-3_require" id="question-3" value="65-1" title="65-1">Cite Text
*   <input type="radio" name="question-3_require" id="question-3" value="65-2" title="65-2">Full Text
*   <input type="radio" name="question-3_require" id="question-3" value="65-3" title="65-3">Get Text
*   <input type="radio" name="question-3_require" id="question-3" value="65-4" title="65-4">RefText
*   <input type="radio" name="question-3_require" id="question-3" value="65-5" title="65-5">Not sure

</td>

</tr>

<tr>

<td align="left">**4\. In considering the following article citation, what does _64_(20) represent?  

Kors, A. C. (1998). Morality on today's college campuses: The assault upon liberty and dignity. _Vital Speeches of the Day, 64_(20), 633-637.**

*   <input type="radio" name="question-4_require" id="question-4" value="66-1" title="66-1">The volume and the number of pages in the article
*   <input type="radio" name="question-4_require" id="question-4" value="66-2" title="66-2">The volume and issue number of the article
*   <input type="radio" name="question-4_require" id="question-4" value="66-3" title="66-3">The year and issue of the article
*   <input type="radio" name="question-4_require" id="question-4" value="66-4" title="66-4">The volume and starting page number of the article
*   <input type="radio" name="question-4_require" id="question-4" value="66-5" title="66-5">Not sure

</td>

</tr>

<tr>

<td align="left">**5\. In an online database which combination of keywords below would retrieve the greatest number of records?**

*   <input type="radio" name="question-5_require" id="question-5" value="143-1" title="143-1">cognition and emotion
*   <input type="radio" name="question-5_require" id="question-5" value="143-2" title="143-2">cognition or emotion
*   <input type="radio" name="question-5_require" id="question-5" value="143-3" title="143-3">cognition not emotion
*   <input type="radio" name="question-5_require" id="question-5" value="143-4" title="143-4">cognition and emotion not feelings
*   <input type="radio" name="question-5_require" id="question-5" value="143-5" title="143-5">Not Sure

</td>

</tr>

<tr>

<td align="left">**6\. If you find a very good article on your topic, what is the most efficient source for finding **related** articles?**

*   <input type="radio" name="question-6_require" id="question-6" value="148-1" title="148-1">An Academic Search Premier database search
*   <input type="radio" name="question-6_require" id="question-6" value="148-2" title="148-2">Bibliography from the article
*   <input type="radio" name="question-6_require" id="question-6" value="148-3" title="148-3">Library Catalog search
*   <input type="radio" name="question-6_require" id="question-6" value="148-4" title="148-4">Other issues / volumes of the journal
*   <input type="radio" name="question-6_require" id="question-6" value="148-5" title="148-5">Not Sure

</td>

</tr>

<tr>

<td align="left">**7\. What is an empirical study?**

*   <input type="radio" name="question-7_require" id="question-7" value="414-1" title="414-1">A survey of previously published literature on a particular topic to define and clarify a particular problem
*   <input type="radio" name="question-7_require" id="question-7" value="414-2" title="414-2">A study based on facts and systematic observation rather than theory or principle
*   <input type="radio" name="question-7_require" id="question-7" value="414-3" title="414-3">Statistical analysis of previously published data
*   <input type="radio" name="question-7_require" id="question-7" value="414-4" title="414-4">A survey of previously published literature that comprehensively identifies, appraises and synthesizes all relevant literature to address a specific question
*   <input type="radio" name="question-7_require" id="question-7" value="414-5" title="414-5">Not Sure

</td>

</tr>

<tr>

<td align="left">**8\. Which area of the SJLibrary.org web site provides a list of core databases for different student majors?**

*   <input type="radio" name="question-8_require" id="question-8" value="164-1" title="164-1">The Academic Gateway
*   <input type="radio" name="question-8_require" id="question-8" value="164-2" title="164-2">Online Tutorials
*   <input type="radio" name="question-8_require" id="question-8" value="164-3" title="164-3">E-journals
*   <input type="radio" name="question-8_require" id="question-8" value="164-4" title="164-4">SJSU Research Topics
*   <input type="radio" name="question-8_require" id="question-8" value="164-5" title="164-5">Not Sure

</td>

</tr>

<tr>

<td align="left">**9\. What does the following citation represent:  

Erzen, J. N. (2007). Islamic aesthetics: An alternative way to knowledge. _Aesthetics and Art Criticism, 65_(1), 69-75.**

*   <input type="radio" name="question-9_require" id="question-9" value="181-1" title="181-1">Book review
*   <input type="radio" name="question-9_require" id="question-9" value="181-2" title="181-2">Journal article
*   <input type="radio" name="question-9_require" id="question-9" value="181-3" title="181-3">Literature review
*   <input type="radio" name="question-9_require" id="question-9" value="181-4" title="181-4">Periodical
*   <input type="radio" name="question-9_require" id="question-9" value="181-5" title="181-5">Not Sure

</td>

</tr>

<tr>

<td align="left">**10\. If you are searching for a book or article your library does not own, you can get a free copy through:**

*   <input type="radio" name="question-10_require" id="question-10" value="183-1" title="183-1">Google Scholar
*   <input type="radio" name="question-10_require" id="question-10" value="183-2" title="183-2">Article Express
*   <input type="radio" name="question-10_require" id="question-10" value="183-3" title="183-3">Interlibrary Services (ILLiad)
*   <input type="radio" name="question-10_require" id="question-10" value="183-4" title="183-4">Webloan
*   <input type="radio" name="question-10_require" id="question-10" value="183-5" title="183-5">Not Sure

</td>

</tr>

<tr>

<td align="left">**11\. How would you locate the hard-copy material for this citation?  

Erzen, J. N. (2007). Islamic aesthetics: An alternative way to knowledge. _Aesthetics and Art Criticism, 65_(1), 69-75.**

*   <input type="radio" name="question-11_require" id="question-11" value="201-1" title="201-1">Search the library catalog for the name of the journal
*   <input type="radio" name="question-11_require" id="question-11" value="201-2" title="201-2">search the library catalog for the author of the article
*   <input type="radio" name="question-11_require" id="question-11" value="201-3" title="201-3">Search Academic Search Premier
*   <input type="radio" name="question-11_require" id="question-11" value="201-4" title="201-4">Search RefWorks
*   <input type="radio" name="question-11_require" id="question-11" value="201-5" title="201-5">Not sure

</td>

</tr>

</tbody>

</table>

## Appendix B

### Cognitive interview script

Let me tell you a little bit about what we're doing. We're testing a new questionnaire with the help of students like you. Our goal is to get a better idea of how the questionnaire and corresponding multiple choice answers are working as far as clarity and understanding. So I'd like you to think aloud as you consider them. Tell me everything you are thinking about as we go over each one.

At times I'll stop to ask you more questions about terms or phrases in the questions and what you think the question is asking about. I'll also take notes.

Please keep in mind that I really want to hear all of your opinions and reactions. Don't hesitate to speak up whenever something seems unclear. It does not matter whether you know the correct answers to the questions or not. This is not a test.

Do you have any questions before we start?

## Appendix C

### Information literacy assessment script

As a student recipient of library instruction, you are being asked to complete an online survey prior to and after today's scheduled library instruction session. Your survey responses are anonymous and will not be tied in any way to your personal identity. Also, you are not being graded so please be honest in answering the questions. This will ensure that the results will assist librarians in developing more useful instructional strategies to support a diverse range of learning styles.

Your consent to participate is voluntary. No services of any kind, to which you are otherwise entitled, will be lost or jeopardized if you choose not to participate.

The survey should take about 5 – 10 minutes to complete. When you are done, minimize the browser window and open another browser window for the instruction session. For those who wish to participate, please pull up the survey from the open browser window on your task bar.

We greatly appreciate your help in making our instructional services better!

## Appendix D

### Participant _Not Sure_ responses

<table width="100%" border="1" cellspacing="0" cellpadding="6" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd">

<tbody>

<tr>

<th>Question</th>

<th>No. of pre-test incorrect answers</th>

<th>No. of post-test incorrect answers</th>

</tr>

<tr>

<td>**1.** Imagine you have an assignment to write a paper based on scholarly information. Which would be the most appropriate source to use?</td>

<td>6 (includes 3 _not sures_)</td>

<td>0</td>

</tr>

<tr>

<td>**2.** How can you tell you are reading a popular magazine?</td>

<td>25 (includes 12 _not sures_)</td>

<td>7 (includes 0 _not sures_)</td>

</tr>

<tr>

<td>**3.** What is the name of the linking tool found in SJSU databases that may lead you to the full text of an article?</td>

<td>22 (includes 18 _not sures_)</td>

<td>10 (includes 1 _not sure_)</td>

</tr>

<tr>

<td>**4.** In considering the following article citation, what does _64_(20) represent?  

Kors, A. C. (1998). Morality on today's college campuses: The assault upon liberty and dignity. _Vital Speeches of the Day, 64_(20), 633-637.</td>

<td>13 (includes 4 _not sures_)</td>

<td>7 (includes 0 _not sures_</td>

</tr>

<tr>

<td>**5.** In an online database which combination of keywords below would retrieve the greatest number of records?</td>

<td>50 (includes 6 _not sures_)</td>

<td>29 (includes 0 _not sures_)</td>

</tr>

<tr>

<td>**6.** If you find a very good article on your topic, what is the most efficient source for finding related articles?</td>

<td>44 (includes 10 _not sures_)</td>

<td>23 (includes 1 _not sure_)</td>

</tr>

<tr>

<td>**7.** What is an empirical study?</td>

<td>35 (includes 7 _not sures_)</td>

<td>20 (includes 1 _not sure_)</td>

</tr>

<tr>

<td>**8.** Which area of the SJLibrary.org web site provides a list of core databases for different student majors?</td>

<td>41 (includes 21 _not sures_)</td>

<td>35 (includes 1 _not sures_)</td>

</tr>

<tr>

<td>**9.**What does the following citation represent:  

Erzen, J. N. (2007). Islamic aesthetics: An alternative way to knowledge. _Aesthetics and Art Criticism, 65_ (1), 69-75.</td>

<td>26 (includes 14 _not sures_)</td>

<td>14 (includes 4 _not sures_)</td>

</tr>

<tr>

<td>**10.** If you are searching for a book or article your library does not own, you can get a free copy through:</td>

<td>42 (includes 34 _not sures_)</td>

<td>12 (includes 3 _not sures_)</td>

</tr>

<tr>

<td>**11.** How would you locate the hard-copy material for this citation?  

Erzen, J. N. (2007). Islamic aesthetics: An alternative way to knowledge. _Aesthetics and Art Criticism, 65_ (1), 69-75.</td>

<td>55 (includes 27 _not sures_)</td>

<td>25 (includes 4 _not sures_)</td>

</tr>

</tbody>

</table>

