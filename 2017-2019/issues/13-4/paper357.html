<!DOCTYPE html>
<html>
<head>
		<title>Information quality assessment on the Web - an expression of behaviour</title>
		<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
		<link rel="stylesheet" href="style.css">
		<link rev="made" href="mailto:t.d.wilson@shef.ac.uk"> 
		<meta name="dc.title" content="Information quality assessment on the Web - an expression of behaviour">
		<meta name="dc.creator" content="Noa Fink-Shamit, Judit Bar-Ilan">
		<meta name="dc.subject" content="Information quality assessment on the Web">
		<meta name="dc.description" content="Introduction. The quality of information on the Web is highly variable, thus users should assess the quality of the information by themselves. Our aim was to elucidate the existence of such quality assessment by studying users carrying out specific tasks on the Web. Method. Seventy-seven individuals were observed performing three tasks, were then interviewed and also completed questionnaires. Analysis. Content analysis and descriptive statistics were used to identify quality assessment components and their attributes. The number of times an attribute was mentioned by the participants was monitored and counted. Results. Information quality assessment is composed of four components: credibility of content, credibility of site, predictive relevance and veracity assessment. Each component consists of several attributes. The extent of attribute use, and by extension of component use, represents an actual behaviour. Conclusions. Information quality assessment is an integral part of Web search behaviour, as all participants evaluated the information and performed a basic assessment using the authority attributes, allowing us to define these attributes as the core elements for the assessment of quality. "> 
		<meta name="dc.subject.keywords" content="">
		<meta name="robots" content="all">
		<meta name="dc.publisher" content="Professor T.D. Wilson">
		<meta name="dc.coverage.placename" content="global">
		<meta name="dc.type" content="text">
		<meta name="dc.identifier" scheme="ISSN" content="1368-1613">
		<meta name="dc.identifier" scheme="URI" content="http://InformationR.net/ir/13-4/paper357.html">
		<meta name="dc.relation.IsPartOf" content="http://InformationR.net/ir/13-4/infres134.html">
		<meta name="dc.format" content="text/html">
		<meta name="dc.language" content="en">
		<meta name="dc.rights" content="http://creativecommons.org/licenses/by-nd-nc/1.0/">
		<meta name="dc.date.available" content="2008-12-15">		
</head>
<body>
<h4 id="vol-13-no-4-december-2008">vol. 13 no. 4, December, 2008</h4>
<h4 id="noa-fink--shamit-and-judit-bar-ilan"><a href="mailto:noaf@savion.huji.ac.il">Noa Fink- Shamit</a> and <a href="mailto:barilaj@mail.biu.ac.il">Judit Bar-Ilan</a></h4>
<p>Department of Information Science, Bar-Ilan University, Ramat-Gan, 52900, Israel</p>
<h4>Abstract</h4>
<blockquote>
<p><strong>Introduction.</strong> The quality of information on the Web is highly variable, thus users should assess the quality of the information by themselves. Our aim was to elucidate the existence of such quality assessment by studying users carrying out specific tasks on the Web.<br>
<strong>Method.</strong> Seventy-seven individuals were observed performing three tasks, were then interviewed and also completed questionnaires.<br>
<strong>Analysis.</strong> Content analysis and descriptive statistics were used to identify quality assessment components and their attributes. The number of times an attribute was mentioned by the participants was monitored and counted.<br>
<strong>Results.</strong> Information quality assessment is composed of four components: credibility of content, credibility of site, predictive relevance and veracity assessment. Each component consists of several attributes. The extent of attribute use, and by extension of component use, represents an actual behaviour.<br>
<strong>Conclusions.</strong> Information quality assessment is an integral part of Web search behaviour, as all participants evaluated the information and performed a basic assessment using the authority attributes, allowing us to define these attributes as the core elements for the assessment of quality.</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>Modern society is an information consuming society and as such the question of information quality is of central importance. Many users consider the Web to be a virtual library and, therefore, when an information need arises they use the Web by default (<a href="#Sch98">Scholz-Crane 1998</a>; <a href="#Gri01">Grimes and Boening 2001</a>). These users fail to appreciate that the inherent structure of the Web has created a new environment (<a href="#Bur01">Burbules 2001</a>); that is, they do not completely internalize the difference between the publication process of printed articles and those published on the Internet. In the conventional publishing process the information not only goes through quality assessment, but it is also subject to a publication policy whereas, over the Internet, the publishing process allows almost anyone to quickly and easily publish his or her opinions. Therefore, the information retrieved does not always originate from recognized or qualified sources and possesses varying information qualities (<a href="#Rieh07">Rieh and Danielson 2007</a>; <a href="#Met07">Metzger 2007</a>) compelling the user to perform an independent assessment of information each time s/he searches for information on the Internet.</p>
<p>Quality is an elusive concept; its definition is situational and has a dual character. It is of a transcendent quality (essence) synonymous with excellence, &quot;an essence that is neither mind nor matter, but a third entity independent of the two &quot;.. and &quot;[e]ven though quality cannot be defined, you know what it is.&quot;(<a href="#Pir74">Pirsig, 1974: 185, 213</a>), while, at the same time quality is a precise and appraised quality that may be looked upon as a product (<a href="#Gar88">Garvin 1988: 40-46</a>).</p>
<p>In the context of this paper the quality of information is solely dependent on whether the information is true, or in other words, whether quality equals veracity. As truth can be subjective (<a href="#Jac02">Jacoby 2002</a>), here information can be accepted as true only when it conforms to an accepted standard or a pattern (<a href="#Tru07">Merriam-Webster Online Dictionary 2007</a>), defined by the scientific community or by the searcher's community at large. As the appraisal of facts and determining what is true are based on personal prior knowledge, a state that is seldom present (<a href="#Kah82">Kahneman and Tversky 1982</a>); therefore to assess the quality of the information, one has to rely on a surrogate mechanism, namely that of credibility. Credibility is &quot;the quality or power of inspiring belief&quot; (<a href="#Cre">Merriam-Webster Online Dictionary 2007</a>) and therefore conveys the impression that it is more a perception (<a href="#Fogg01">Fogg <em>et al.</em> 2001</a>; <a href="#Fre04">Freeman and Spyridakis 2004</a>; <a href="#Liu04">Liu 2004</a>) than an actual appraisal of facts. By contrast, in the context of the current study, credibility is not a perception nor it is a criterion for relevance judgment (<a href="#bar94">Barry 1994</a>; <a href="#Bat99">Bateman 1999</a>), it is a direct measure of actual quality (<a href="#Gar88">Garvin 1988</a> : 40-46), re-created each time the question of truth arises. Hence, assigning credibility to information is not the end; rather, it is the means of accepting the veracity of information.</p>
<p>Patrick Wilson's (<a href="#Wil83">1983</a>) epistemic authority is another theoretical concept that is closely linked to credibility, which was discussed by both Rieh (<a href="#Rieh02">2002</a>) and Savolainen (<a href="#Sav07">2007</a>) in the context of Web searching. Cognitive authority is a quantitative concept that consists of two components, competence and trustworthiness, and is viewed as a relationship between two entities, where there has to be an acknowledgement of the authority of one entity over the other. Acceptance of the information conveyed by the authoritative party implies that cognitive authority can be found in various sources and not only in individuals. It isimportant, however, to note that one can be an expert without being regarded as an authority or be regarded as authority in one area, but not is another area (<a href="#Wil83">Wilson 1983: 13-15</a>).</p>
<p>The concept of credibility was discussed in two extensive literature reviews (<a href="#Met03">Metzger <em>et al.</em> 2003</a>; <a href="#Rieh07">Rieh and Danielson 2007</a>). Based on surveyed populations, several studies tried to answer the questions how people evaluate Websites and their content and, if they do, what evaluation criteria are employed, how and to what extent these criteria are used (<a href="#Fla00">Flanagin and Metzger 2000</a>; <a href="#Liu04">Liu 2004</a>; <a href="#Liu05">Liu and Huang 2005</a>; <a href="#Fogg01">Fogg <em>et al.</em> 2001</a>; <a href="#Bar03">Barnes <em>et al.</em> 2003</a>).It seems that, when assessing credibility, attributes that pertain to the site are used more readily (<a href="#Fogg03">Fogg <em>et al.</em> 2003</a>) than attributes relating to content, implying that sites whose looks are more appealing are perceived as holding more credible content (<a href="#Rob08">Robins and Holmes 2008</a>). However, there exist other studies (<a href="#Rie02">Rieh 2002</a>; <a href="#Sta02">Stanford <em>et al.</em> 2002</a>; <a href="#Eys02">Eysenbach and Köhler 2002</a>), which consider attributes like author and source as key elements for evaluating the quality information, thus disassociating themselves from appearance and converging to the conventional pattern of assessing information.</p>
<p>People regard the Web as an important source of information and turn to it by default (<a href="#Gri01">Grimes and Boening 2001</a>). Although users acknowledge the need to evaluate the quality of the information retrieved (<a href="#Ucl03">The UCLA Internet Report - Year Three, 2003: 39</a>), they do not always do so (<a href="#Sch98">Scholz-Crane 1998</a>; <a href="#Gra03">Graham and Metaxes 2003</a>), nor do they corroborate the retrieved information with other sources (<a href="#Fla00">Flanagin and Metzger 2000</a>; <a href="#Fal04">Fallis 2004</a>); therefore, studying methods or mechanisms whereby information quality assessment can be performed is of utmost importance.</p>
<p>Several methods are used by librarians and other information professionals that can be seen as substitutes for a quality control system:</p>
<ul>
<li>The <em>guideline</em> or the checklist method ( <a href="#Ale99">Alexander and Tate 1999</a>).</li>
<li>The <em>fact checking</em> method presupposes that credibility and quality of any information obtained is dubious until corroborated with other sources (<a href="#Bur01">Burbules 2001</a>).</li>
<li>The <em>contextual approach</em> is based on using external information to the current site. That is, examining it by using three basic techniques; teaching where to locate quality sources on the Web, then comparing and corroborating the information with other sources(<a href="#Meo04">Meola 2004</a>).</li>
<li>The <em>common sense approach</em> can be considered a learning process triggered by the specific information need. The knowledge accumulated during this learning process allows the user to assess the quality of information (<a href="#Haas03">Haas and Wearden 2003</a>).</li>
</ul>
<p>Previous studies were inconclusive on the question whether users voluntarily assess the quality of information accessible on the Web. We hypothesize that as an integral part of Web information behaviour there exist what we may call <em>information quality assessment</em> which represents a true quality control measure that utilizes specific tools and procedures to assist the assessment the quality of information. In contrast, however, to any quality control programme, which is performed by a specially designated individual, in the case of information retrieval, quality control is performed by the user him/herself as a <em>reflex</em> reaction to the diverse and inconsistent quality of information.</p>
<h3 id="purpose-of-the-study">Purpose of the study</h3>
<p>The purpose of this study is to investigate information quality assessment of information retrieved on the Web by directly observing users' information seeking behaviour and by asking the following questions:</p>
<ul>
<li>Does Web search behaviour include information quality assessment?</li>
<li>To what extent do people engage in quality assessment?</li>
<li>What are the main attributes used for assigning quality to information?</li>
</ul>
<h2 id="methods">Methods</h2>
<p>To answer our research questions we used several instruments; direct observation, interviews, open and closed ended questionnaires and the <em>think aloud</em> technique. Actions taken during tasks were monitored by using the surveillance software <a href="http://www.webcitation.org/5d8ailvvn">Spybuddy.</a></p>
<h3 id="the-study-population">The study population</h3>
<p>The study population was recruited between November 2005 and February 2006 by advertising on university bulletin boards using the purposeful sampling method as described by Patton (<a href="#Pat90">1990: 182</a>). When a person was recruited, s/he signed an informed consent, participated in a full research session and was then offered coupons for the university bookstore. The recruited study population was composed of seventy-seven individuals recruited from the general student (undergraduates and graduates) population at Bar-Ilan University. The majority were females (72.7% out of 77 participants), native Hebrew speakers (89.6%), who prefer to search the Web in Hebrew (79.2%) and are very frequent Web users (as 86.1% of them indicated that they use the Internet every day).</p>
<h3 id="sessions">Sessions</h3>
<p>Each session started with briefly explaining that we are conducting a study about Web search behaviour, without disclosing that we are actually looking for behaviour related to information quality assessment. Then the think aloud technique (<a href="#Rie02">Rieh 2002</a>) was demonstrated, an informed consent form was signed and a background questionnaire was completed by the participants. Thereafter, the session started with each participant completing two search and one evaluation tasks.</p>
<p>Here we only discuss the results of one of the search tasks denoted as scenario task, as this task simulates best the natural Web information behaviour (searching for information on a general topic). One of the additional tasks, the domain task, intended to disclose how domain knowledge influenced information quality assessment. The evaluation task was devised to observe isolated information quality assessment behaviour.</p>
<p>The topic for the scenario task was selected from the site <a href="http://www.about.com">about</a> as suggested by Toms and Taves (<a href="#Toms04">2004</a>) and the topic was social phobia. The scenario read as follows:</p>
<blockquote>
<p>You have a close friend who is shy, has problems with adjusting to new situations (like new employment, dating the opposite sex) and expresses anxiety and fear when s/he has to leave the safety of home. You, as his/her best friend encourage him/her to seek professional help. The diagnosis provided was that your friend suffers from Social Phobia or Social Anxiety. Overwhelmed by this information you try to find more information on the subject using the Web.</p>
</blockquote>
<p>After completing each search task participants were interviewed for three to five minutes. They were asked for their comments, the reasons for choosing or not choosing specific sites and how they usually selected information from the Web. The responses were recorded and later transcribed(<a href="#Pat90">Patton 1990: 348-351</a>). Then after performing all three tasks, the participants were asked to complete a final questionnaire. It included open and closed questions relating to their views about the quality of information on the Web and the methods employed by them to ascertain it.</p>
<h3 id="categorization">Categorization</h3>
<p>Based on ten randomly chosen transcripts and using content analysis (<a href="#Str90">Strauss and Corbin 1990</a>), a hierarchical tree of categories (later to be referred as components) with their relevant attributes was constructed. Using a Microsoft Excel table, each participant was identified by an ordinal number, with the results of each task for each participant (scenario, domain, and evaluation) recorded in a separate row, while attributes and sub-attributes were placed under a different column. Each time a participant noted an attribute or a sub-attribute the value one (1) was marked in the appropriate cell ( <a href="#Mil94">Miles and Huberman, 1994: 43-45, 253</a>). After accepting the emerging tree of categories, transcripts of the remaining sixty-seven participants were similarly analyzed and the categorization tree was expanded and altered as the data unfolded ( <a href="#Str90">Strauss and Corbin 1990: 61-76</a> ).</p>
<h4 id="categorization-reliability">Categorization reliability</h4>
<p>On a randomly selected 10% of the study population, categorization was corroborated by two other coders ( <a href="#Mer99">Merrick 1999</a>). Only after verifying that there was an agreement between the external coders and the categorization performed by us, was the overall categorization accepted.</p>
<h4 id="scoring-method">Scoring method</h4>
<p>The scoring method expresses numerically the use of the information quality assessment components and its attributes, where an attribute is a feature of an item mentioned by participants. Each component consisted of a different number of attributes, and in order to be able to compare them the components were normalized, i.e., divided by the number of attributes of each component. There were two types of scores:</p>
<ul>
<li>Normalized attribute score: is the number of participants who mentioned the attribute (called attribute score) divided by the number of participants (seventy-seven) and presented as percentage.</li>
<li>Normalized group component score: is the sum all the attribute scores for a given component divided by the number of participants and normalized by dividing it by the number of attributes of the component, and then presented as percentage.</li>
</ul>
<h2 id="results">Results</h2>
<h3 id="the-information-quality-assessment-components-and-attributes">The information quality assessment components and attributes</h3>
<p>The attributes mentioned and used by the participants, defined four different evaluative components (Figure 1): <em>credibility of site</em>, <em>credibility of content</em>, <em>predictive relevance</em> and <em>veracity assessment</em>. In the following subsections we describe the attributes of each of these components. These four components comprise the core category information quality assessment.</p>
<div align="center">![Figure 1 The information quality assessment components](p357fig1.jpg)</div>
<div align="center">  
**Figure 1: The information quality assessment components**</div>
<h3 id="attributes-that-comprise-the-component-credibility-of-content">Attributes that comprise the component <em>credibility of content</em></h3>
<p>There is consensus that, for assessing credibility (referred to here as <em>credibility of content</em> (Figure 2)), some attributes are considered as core elements and, therefore, one expects that every credibility assessment will take them into account ( <a href="#Rie07">Rieh and Danielson 2007</a>). These are: scope, accuracy, objectivity, currency and authority. However, in reality not all attributes are used ( <a href="#Fogg03">Fogg <em>et al.</em> 2003</a>) and users usually mention only one or two of them ( <a href="#Sch98">Scholz-Crane 1998</a>). In our study, users mentioned and used extensively only the authority attributes.</p>
<p>Previous studies found that attributes that relate to authority are the main attributes people use when evaluating information quality ( <a href="#Rie02">Rieh 2002</a>; <a href="#Fre04">Freeman and Spyridakis 2004</a>; <a href="#Eys02">Eysenbach and Köhler 2002</a>; <a href="#Liu05">Liu and Huang 2005</a>) and, therefore, it is not surprising that in the <em>scenario task</em> more than 60% of the study population mentioned the attributes <em>source</em> and <em>author</em> where the first (mentioned by 62% of the participants) is expressed by three sub-attributes: public, private and commercial as nicely verbalized by three participants '<em>I see that it is Geocities.</em>' '<em>It is exactly what I looked for, a non profit organization.</em>' '<em>Although I see it's a .com site, it can be useful</em>'.</p>
<p>The attribute <em>author</em> (mentioned by 70% of the participants) refers to the author's credentials, qualifications (<a href="#Met07">Metzger 2007</a>), as was stated by two participants: '<em>.the person who wrote it is a doctor. I don't know if this doctor really exists, yet the information is helpful</em>'. '<em>I am interested knowing who the author is... whether he's an expert or someone who decided to publish his thoughts</em>'. However, the fact that participants noticed the author's name or credentials does not mean that they actually verified it.</p>
<p>Similarly to the findings of Flanagin and Metzger (<a href="#Fla00">2000</a>), under the normative Web search behaviour (<em>scenario task</em>) we find that the attributes; <em>accuracy</em>, <em>currency</em>, <em>objectivity</em> were mentioned by very few participants. The low usage of the attribute <em>accuracy</em> (In the context of this study, accuracy, translated from the Hebrew, exists only in the sense of exactness and not that of truth) is explained by the participants themselves when stating that '<em>.I really can't say whether the information is accurate or complete as I didn't study 'academically' social phobia .if I were a psychologist then I would know</em>'.</p>
<p>The attributes <em>accuracy and currency</em> as well as the sub-attribute <em>links and references</em> - refers to whether the links or the references themselves are current: '<em>the site presents how to deal with social phobia but the links that the site refers to are not from recent years.</em>' were also not used by many participants, as reflected in Metzger's (<a href="#Met07">2007: 2083</a>) conclusion that '<em>people do not seem to take the currency of the information they find online into account when making credibility judgments</em>'.</p>
<p>Other frequently used attributes were: <em>prior acquaintance with site</em> (25%), <em>writing style</em> (26%), <em>type of reference</em> (35%) and <em>scope and completeness</em> (27%). The use of the attribute <em>prior acquaintance with site</em> is interesting as it indicates that a site has been already assessed for quality, found to be of good quality, implying that those sites have an advantage compared to other (unknown sites) as there is less need to go though the information quality assessment process and as one of the participants stated: '<em>I am familiar with <a href="http://www.infomed.co.il">InfoMed</a> it is a good source</em>'.</p>
<p>When evaluating an article or a book, one turns to examine whether the author included current references and what types of references were used. This behaviour was found by Eysenbach and Köhler (<a href="#Eys02">2002</a>) and by the current research as well. The participants used the attribute <em>writing style</em> and its sub attributes <em>scientific</em> or <em>popular</em> as a <em>sign</em> of quality (similar to <a href="#Eys02">Eysenbach and Köhler 2002</a>) as one of our participants stated: '<em>It's a serious article; the language is more scientific</em>'.</p>
<p>The literature ( <a href="#Met07">Metzger 2007</a>; <a href="#Rie07">Rieh and Danielson 2007</a>) presents scope as one of the five core attributes and, indeed, Scholz-Crane's (<a href="#Sch98">1998</a>) findings indicated this criterion should be considered as one of the most important criteria for evaluation information quality. However, in contrast to the extensive use of the authority attributes (author and source) by the majority of participants (over 60% of them) only 27% of our participants used the attribute <em>scope</em> and <em>completeness</em> preventing us from ascribing this attribute as one of the core attributes.</p>
<div align="center">![Figure 2: The attributes that make up the component credibility of content](p357fig2.jpg)</div>
<div align="center">  
**Figure 2: The attributes that make up the component _credibility of content_**</div>
<h3 id="attributes-that-comprise-the-component-credibility-of-site">Attributes that comprise the <em>component credibility</em> of site</h3>
<p>Previous studies suggested that the appearance of a site and its design are pivotal when evaluating the overall credibility of that site. However, when prior knowledge about the subject exists (<a href="#Eas01">Eastin 2001</a>; <a href="#Rie02">Rieh 2002</a>; <a href="#Sta02">Stanford <em>et al.</em> 2002</a>) people emphasize the evaluation of the quality of the information (<a href="#Liu05">Liu and Huang 2005</a>; <a href="#Sta02">Stanford <em>et al.</em> 2002</a>; <a href="#Jen03">Jenkins <em>et al.</em> 2003</a>) more than the appearance of the site.</p>
<p>Attributes that were mentioned and used are presented in Figure 3, where the main attributes (coloured magenta) of credibility of site are <em>design</em> and <em>language</em>. The attribute <em>design</em> is expressed by several sub attributes:</p>
<ul>
<li><span style="color: maroon;">picture and figures</span>: '<em>The picture is very appealing . the pictures add a lot</em>.' </li>
<li><span style="color: maroon;"> contact </span>: <em>There is an option to contact the psychologist.</li>
	
<li><span style="color: maroon;">advertisement</span>: <em>There are no ads so it increases the positive opinion about the site.</li>  
	
<li><span style="color: maroon;">number of links </span>: <em>It is worth reading; there are a lot of external links.</li>
<li><span style="color: maroon;">layout of page</span>: <em>The page is divided into subjects and titles.</li>
	
<li><span style="color: maroon;">tables and numbers </span>: <em>There is a table with a test about social phobia.nice that helps.</li>
	
<li><span style="color: maroon;">ease of navigation</span>: <em>...it's a better site than the last one, it is more organized as it is divided into categories...</li>
	
<li><span style="color: maroon;">search engine on site </span>: <em>.there is a search engine..shows that the a Web designer planed the site.</li>
	
<li><span style="color: maroon;">The colour here is problematic - too much.</li>
	
<li><span style="color: maroon;">The site was updated a few months ago.</li>	
	
<li><span style="color: maroon;">The font is too small.</li></ul>
	
</ul>

<p>The attribute <em>language</em> depicted herein denotes a preconceived idea that sites written in English are more academic that those written in Hebrew as expressed by one of the participants '<em>Because the terminology is in English I will choose this site. it looks more scientific</em>'.</p>
<p>Barnes <em>et al.</em> ( <a href="#Bar03">2003</a>) found that when participants evaluate Websites they relied more on the design and aesthetics criterion, than on the authority attributes while the present study indicates that only 19% and 23% of the participants used the attributes <em>tables and numbers</em> and <em>layout</em> respectively. Freeman and Spyridakis ( <a href="#Fre04">2004</a>) concluded that the presence or absence of links did not significantly affect credibility ratings; which may explain the fact that only 31% of our participants used the attribute <em>number of links</em>.</p>
<div align="center">![Figure 3: The attributes that make up the component credibility of site](p357fig3.jpg)</div>
<div align="center">  
**Figure 3: The attributes that make up the component _credibility of site_**</div>
<h3 id="attributes-that-comprise-the-component-predictive-relevance">Attributes that comprise the component <em>predictive relevance</em></h3>
<p>When looking for information, on one hand, one looks for the most relevant information to a specific query and topic (<a href="#Bat99">Bateman 1999</a>; <a href="#Wan98">Wang and Soergel 1998</a>) while on the other, one needs to reduce the amount of information retrieved. Although predictive relevance is conceptually related to relevance, it is also akin to Hogarth's ( <a href="#Hog87">1987</a>) predictive judgment as the users judge the relevance of the search results before accessing the site.</p>
<p>The use of this component (Figure 4) suggests that the users predict the relevance of the search results based on the <em>ranking</em> of the results by the search engines, the <em>snippets</em> and other characteristics of the search results like the title of the retrieved document and the occurrence of the <em>query terms</em> in the title, in the snippet or in the <em>url</em> of the search result, as explained by two participants: '<em>the first results are most relevant so I usually look at them</em>', '<em>I look at the title and the abstract (snippet) to see if it matches what I asked.</em>' , '<em>I usually look at the URL, and then I can see that it has no relevancy to social anxiety [because the query terms do not appear in the url].</em>'</p>
<p>People prefer to use their mother tongue when searching the Web ( <a href="#Eys02">Eysenbach and Köhler 2002</a>) as well as when assigning relevance (<a href="#Han05">Hansen and Karlgren 2005</a>). Therefore, <em>language</em>, an attribute of predictive relevance, serves here as a <em>filter</em> to information as participants not only searched exclusively in Hebrew but if additional information was linked to English sites they automatically exited the site, as expressed by one of the participants '<em>I wouldn't enter this site, because of the language - English</em>'. As can be seen this attribute is different from the attribute <em>language</em> of the component in <em>credibility of site</em>, where there the presence of the attribute <em>language</em> (such as English) increases the credibility and the reputation in the eyes of the users, whereas in the case of the component <em>predictive relevance</em> the attribute <em>language</em> serves as a filter to reduce the amount of relevant sites. Although <em>language</em> has an impact on <em>(predictive) relevance</em> as 18% of our participants used this attribute, the attributes <em>snippet</em> and <em>ranking</em> were the most frequently used attributes of this component, mentioned by 32% and 30% of the participants respectively.</p>
<div align="center">![Figure 4: The component predictive relevance and its attributes](p357fig4.jpg)</div>
<div align="center">  
**Figure 4: The component _predictive relevance_ and its attributes**</div>
<h3 id="attributes-that-comprise-the-component-veracity-assessment">Attributes that comprise the component <em>veracity assessment</em></h3>
<p>It was Descartes who said '<em>if one wishes to know the truth, then one should not believe an assertion until one finds evidence to justify of doing so</em>' ( <a href="#Gil93">Gilbert <em>et al.</em> 1993: 221</a> ), thus positioning truth as pivotal for accepting information. Hence, to ascertain the veracity of information one should 1) compare and corroborate the information retrieved on the Web with other sources (<a href="#Meo04">Meola 2004</a>); 2) ascertain authority; and 3) compare the information retrieved on the Web with personal knowledge (<a href="#Haas03">Haas and Waerden 2003</a>). Therefore, the attributes defining the component veracity assessment (see Figure 5) were <em>previous knowledge</em> (as expressed by a participant '<em>from my knowledge, I know that socio is society.</em>') and <em>corroboration</em>. The latter seems to be the default, whereby one will try to expand the pre-existing knowledge by corroborating the newly acquired information with other sources. Several sub-attributes of <em>corroboration</em> were identified by us:</p>
<ul>
<li><span style="color: maroon;">other sites</span> : '<em>I think I found a credible site but to be sure .I prefer to find two more.</em></li>
<li><span style="color: maroon;">forums</span> - The Web had opened a new and immediate channel of exchanging information while reducing anxieties and uncertainties known as forum as was acknowledged by a participant: '<em>I use forums a lot, like the one on doctors.co.il [an Israeli health site] so I am sure my friend can find useful information there, also the site is managed by known doctors</em>. The use of this attributes is further emphasized by the fact that 45% of the participants used the sub-attribute <em>forums maintained by experts</em> suggesting that in this case too, preference is given to authoritative sources.</li>
<li><span style="color: maroon;">printed sources</span>: '<em>.not enough. needs corroboration with books</em>.</li>
<li><span style="color: maroon;">consultation with experts</span> - This attribute was not utilized by the users during the experiment, but was mentioned by them as a mean for assuring the quality of information in the final questionnaire.</li>
</ul>
<div align="center">![Figure 5: The attributes that make up the component veracity assessment](p357fig5.jpg)</div>
<div align="center">  
**Figure 5: The attributes that make up the component _veracity assessment_**</div>
<h3 id="attribute-and-component-use-defines-behaviour">Attribute and component use defines behaviour</h3>
<p>Attribute scores are the reflection of the number of times an attribute is mentioned by the participants; therefore, the extent and rate of the attribute use, and by extension of component use, defines behaviour. Hence, attributes mentioned by participants during observation are actually used and, therefore, each component and attribute of the information quality assessment process is not a perception it is rather a tool to be utilized for direct appraisal of the quality of information. We envisage the use of information quality assessment as if the user has at his disposal an array of attributes integrated into four different components and utilizes them as the need arises. However, it seems that there is a small preference in usage of certain components as is shown in Table 1 where participants used more readily the components (and therefore its attributes) <em>credibility of content</em> and <em>predictive relevance</em>, more often than the components <em>veracity assessment</em> and <em>credibility of site</em>, suggesting that for our participants, like for participants in other studies, ( <a href="#Sta02">Stanford <em>et al.</em> 2002</a>; <a href="#Rie02">Rieh 2002</a>) the information has to be both credible and relevant, before being assessed for its veracity or presentation. Note that the <em>normalized group score</em> measures the extent of the use of a specific component on average, i.e., the participants in this experiment used on average 16.6% of the attributes available for the component <em>credibility of content.</em>.</p>
<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #444444 solid; border-top: #444444 solid; font-size: smaller; border-left: #444444 solid; border-bottom: #444444 solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #ffffff"> 
		<caption align="bottom"><br><strong>**Table 1: Participants use more readily the components _credibility of content_ and _predictive relevance_**</caption>
<tbody>
<tr>
<th>Component</th>
<th>Normalized group component score  
(N=77)</th>
</tr>
<tr>
<td>Credibility of content</td>
<td align="center">16.6%</td>
</tr>
<tr>
<td>Predictive relevance</td>
<td align="center">13.7%</td>
</tr>
<tr>
<td>Veracity assessment</td>
<td align="center">9.2%</td>
</tr>
<tr>
<td>Credibility of site</td>
<td align="center">7.2%</td>
</tr>
</tbody>
</table>
<h2 id="conclusions">Conclusions</h2>
<p>Based on our data, we suggest that information quality assessment is an integral part of the overall Web search behaviour, as all participants performed to some extent a kind of information quality assessment. Information quality assessment is intimately linked to the basic need of people to ascertain the truth; it is composed of components and attributes that represent an actual behaviour. Hence, attributes mentioned by participants during sessions are actually used and, therefore, each component and attribute of the information quality assessment process is not a perception but rather a tool to be used for direct appraisal of the quality of information.</p>
<p>Similar to other reports ( <a href="#Met07">Metzger 2007</a>), we also found that, when evaluating quality, participants tend to use only few of the conventional evaluation criteria. Previous studies showed that users link credibility to authority ( <a href="#Rie02">Rieh 2002</a>; <a href="#Fogg03">Fogg <em>et al.</em> 2003</a>; <a href="#Eys02">Eysenbach and Köhler 2002</a>; <a href="#Liu04">Liu 2004</a>). Similarly, the main finding of this study is that the participants, regardless of their previous knowledge on the subject or experience in Web use, use the attributes <em>author</em> and <em>source</em> as primary attributes to assess quality. It seems as if in order to evaluate quality the use of these attributes not only lessens the need to use other attributes, but it constitutes a natural behaviour were one tries to assess truth by asking questions like: Who said it? Who wrote it? What is the source of this information? We have further shown, like Eysenbach and Köhler ( <a href="#Eys02">2002</a>), that, for non native English speakers, language may be an obstacle. We have noticed that <em>language</em> has become an important attribute of predictive relevance together with the attributes <em>ranking</em> and <em>snippets</em>.</p>
<p>The study shows that Web users always, although not always knowingly, perform some rudimentary information quality assessment, where newly introduced Web features such as ranking and snippets are integrated with the classical attributes of authority to generate a new mechanism to ascertain quality.</p>
<h2>Acknowledgements</h2>
<p>Results presented in this paper are part of the Ph.D dissertation of Noa Fink-Shamit, carried out under the supervision of Judit Bar-Ilan at the Department of Information Science, Bar-Ilan University.</p>
<h2>References</h2>
<p>Alexander, J.E. &amp; Tate, M.A. (1999). <em>Web wisdom: how to evaluate and create information quality on the Web</em>. Hillsdale, NJ: Erlbaum.</p>
<p>Barnes, M.D., Penrod, C., Brad, L., Neiger, R.M., Marrill, R.T., Eggett, D.L. <em>et al.</em> (2003). Measuring the relevance of evaluation criteria among health information seekers on the Internet. <em>Journal of Health Psychology</em>, <strong>8</strong> (1), 71-82.</p>
<p>Barry, C.L. (1994).User-defined relevance criteria: an exploratory study . <em>Journal of American Society for Information Science</em>, <strong>45</strong>(3), 149-159.</p>
<p>Bateman, J. (1999). Modeling the importance of end-user relevance criteria. <em>Proceedings of the ASIS Annual Meeting</em> ,<strong>36</strong>, 396-406.</p>
<p>Burbules, N. C. (2001). Paradoxes of the Web: the ethical dimensions of credibility. <em>Library Trends</em>, <strong>49</strong>(3), 441-453.</p>
<p><a id="Cred" name="Cre"></a><a href="http://www.merriam-webster.com/dictionary/credibility">Credibility</a> (2007). In <em>Merriam-Webster Online Dictionary</em>. Retrieved April 1, 2008 from: http://www.merriam-webster.com/dictionary/credibility. (Archived by WebCite® at http://www.webcitation.org/5d8f0iptR)</p>
<p>Eysenbach, G. &amp; Köhler, C. (2002). <a href="http://www.bmj.com/cgi/content/full/324/7337/573">How do consumers search for and appraise health information on the World Wide Web? Qualitative study using focus groups, usability tests, and in-depth interviews.</a> <em>British Medical Journal</em>, <strong>324</strong>, 573-577. Retrieved 15 December, 2008 from http://www.bmj.com/cgi/content/full/324/7337/573 (Archived by WebCite® at http://www.webcitation.org/5d8fNA8GC)</p>
<p>Eastin, M.S. (2001). <a href="http://jcmc.indiana.edu/vol6/issue4/eastin.html">Credibility assessments of online health information: The effects of source expertise and knowledge of content.</a> <em>Journal of Computer-Mediated Communication</em>, <strong>6</strong> (4). Retrieved April, 11 2008 from: http://jcmc.indiana.edu/vol6/issue4/eastin.html (Archived by WebCite® at http://www.webcitation.org/5d8fTjnt0)</p>
<p>Fallis, D. (2004). On verifying the accuracy of information: philosophical perspectives. <em>Library Trends</em>, <strong>52</strong>(3), 463-487.</p>
<p>Flanigan, A. &amp; Metzger, M. (2000). Perceptions of Internet information credibility. <em>Journalism and Mass Communication Quarterly</em>, <strong>77</strong>(3), 515-540.</p>
<p>Fogg, B. J., Marshall, J., Laraki,O., Osipovich, A. Varma, C., Fang, N., Paul, J.,Rangnekar, A ., Shon, J., Swani, P. &amp; Marissa Treinen. (2001). <a href="http://captology.stanford.edu/pdf/p61-fogg.pdf">What makes a Web site credible? A report on a large quantitative study</a>. In <em>Proceedings of ACM CHI 2001 Conference on Human Factors in Computing Systems</em>, vol. 1, (pp. 61-68). New York, NY: ACM Press.</p>
<p>Retrieved 26 April, 2008 from: http://captology.stanford.edu/pdf/p61-fogg.pdf (Archived by WebCite® at http://www.webcitation.org/5d8ffszaE)</p>
<p>Fogg, B. J., Soohoo, C. &amp; Danielson, D.(2002). <a href="http://www.consumerwebwatch.org/dynamic/web-credibility-reports-evaluate-abstract.cfm">How do people evaluate a Web site's credibility: results from a large study</a> . Published by Consumer WebWatch. Retrieved 19 April, 2008 from http://bit.ly/KaO4 (Archived by WebCite® at http://www.webcitation.org/5d8fqA0RX)</p>
<p>Fogg, B. J., Soohoo, C., Danielson, D. R., Marable, L., Stanford, J. &amp; Trauber, E. R. (2003). How do users evaluate the credibility of Web sites? A study with over 2,500 participants. In <em>Designing For User Experiences: Proceedings of the 2003 Conference on Designing for User Experiences</em>, San Francisco, California June 06 - 07, 2003, (pp. 1-15), New York, NY: ACM Press.</p>
<p>Freeman, K. S. &amp; Spyridakis, J.H. (2004). An examination of factors that affect the credibility of online health Information. <em>Technical Communication</em>, <strong>51</strong>(2), 239-263.</p>
<p>Garvin, D.A (1988). <em>Managing quality: the strategic and competitive edge</em>. New York, NY: Free Press.</p>
<p>Gilbert, D.T., Tafarodi, R.W. &amp; Malone, P.S.(1993). <a href="http://www.wjh.harvard.edu/~dtg/Gilbert%20et%20al%20(EVERYTHING%20YOU%20READ).pdf">You can't not believe everything you read</a>. <em>Journal of Personality and Social Psychology</em>, <strong>65</strong>(2), 221-233.Retrieved April 19, 2008 from: http://bit.ly/1SFBq7 (Archived by WebCite® at http://www.webcitation.org/5d8fzaZFi)</p>
<p>Graham, L. &amp; Metaxes, P.T. (2003). Of course it's true; I saw it on the Internet!: critical thinking in the Internet era. <em>Communications of the ACM</em>, <strong>46</strong>(5), 70-75.</p>
<p>Grimes, D.J. &amp; Boening, C.H. (2001). Worries with the Web: a look at student use of Web resources. <em>College and Research Libraries</em>, <strong>62</strong>(1), 11-23.</p>
<p>Haas, C. &amp; Wearden, S.T. (2003). E-credibility: building common ground in Web environments. <em>Educational Studies in Language and Literature</em>, <strong>3</strong>(1/2), 169-184.</p>
<p>Hansen, P. &amp; Karlgren, J. (2005). Effects of foreign language and task scenario on relevance assessment, <em>Journal of Documentation</em>, <strong>61</strong>(5), 623-639.</p>
<p>Hogarth, R. (1987). <em>Judgement and choice.</em> 2nd ed. New York, NY: Wiley.</p>
<p>Jenkins, C., Corritore, C. L. &amp; Wiedenbeck, W. (2003). Patterns of information seeking on the Web: a qualitative study of domain expertise and web expertise. <em>IT and Society</em>, <strong>1</strong>(3), 64-89.</p>
<p>Liu, Z. (2004). Perceptions of credibility of scholarly information on the Web. <em>Information Processing and Management</em>. <strong>40</strong>(6), 1027-1038.</p>
<p>Liu, Z. &amp; Huang, X. (2005). Evaluating the credibility of scholarly information on the Web: a cross cultural study. <em>The International Information and Library Review</em>, <strong>37</strong>(2), 99-106.</p>
<p>Jacoby, M.G. (2002). Kierkegaard on truth. <em>Religious studies</em>, <strong>38</strong>(1), 27-44.</p>
<p>Kahneman. D. &amp; Tversky, A. (1982). Variants of uncertainty. <em>Cognition</em>, <strong>11</strong>(2), 143-157.</p>
<p>Meola, M. (2004). <a href="http://www.tcnj.edu/~meolam/documents/Chucking_003.pdf">Chucking the checklist: a contextual approach to teaching undergradutes Web evaluation.</a> <em>Portal: Libraries and the Academy</em>, <strong>4</strong>(3), 331-344. Retrieved 19 April, 2008 from http://www.tcnj.edu/~meolam/documents/Chucking_003.pdf (Archived by WebCite® at http://www.webcitation.org/5d8gDA3IH)</p>
<p>Metzger, M. J. (2007). Making sense of credibility on the Web: models for evaluating online information and recommendations for future research. <em>Journal of the American Society for Information Science and Technology</em>, <strong>58</strong>(13), 2078-2091.</p>
<p>Metzger, M. J., Flanagin, A. J., Eyal, K., Lemus, D. &amp; McCann, R. (2003). Credibility for the 21st century: integrating perspectives on source, message, and media credibility in the contemporary media environment. <em>Communication yearbook</em>, <strong>27</strong>, 293-335.</p>
<p>Merrick, E (1999).An exploration of quality in qualitative research. In M. Kopala &amp; L.A. Suzuki, (eds.) <em>Using qualitative methods in psychology</em> (pp. 25-36). Thousand Oaks, CA: Sage Publications.</p>
<p>Miles, M.B. &amp; Huberman, A.M. (1994). <em>Qualitative data analysis: an expanded sourcebook</em>. 2nd ed. Thousand Oaks, CA: Sage Publications.</p>
<p>Patton, M.Q. (1990). <em>Qualitative evaluation and research methods</em>. 2nd ed. Newbury Park, CA: Sage Publications.</p>
<p>Pirsig, R.M. (1974). <em>Zen and the art of motorcycle maintenance: an inquiry into values</em>. New York, NY: Bantam Books.</p>
<p>Rieh, S.Y. &amp; Danielson, D.R. (2007). Credibility: a multidisciplinary framework. <em>Annual Review of Information Science and Technology</em>, <strong>41</strong>, 307-364.</p>
<p>Rieh, S.Y. (2002). Judgment of information quality and cognitive authority in the Web. <em>Journal of the American Society for Information Science and Technology</em>, <strong>53</strong>(2), 145-161.</p>
<p>Rieh, S.Y. &amp; Belkin, N.J. (2000). Interaction on the Web: scholars' judgment of information quality and cognitive authority. <em>Proceedings of the ASIS Annual Meeting</em>, <strong>37</strong>, 25-38.</p>
<p>Robins, D &amp; Holmes, J. (2008). Aesthetics and credibility in web site design. <em>Information Processing and Management</em>, <strong>44</strong>(1), 386-399.</p>
<p>Savolainen, R. (2007). <a href="http://InformationR.net/ir/12-3/paper319.html">Media credibility and cognitive authority. The case of seeking orienting information</a>. <em>Information Research</em>, <strong>12</strong>(3), paper 319. Retrieved 19 April, 2008 from http://InformationR.net/ir/12-3/paper319.html (Archived by WebCite® at http://www.webcitation.org/5d8h3ddMQ)</p>
<p>Scholz-Crane, A. (1998). Evaluating the future: a preliminary study of the process of how undergraduate students evaluate Web sources. <em>Reference Services Review</em>, <strong>26</strong>(3/4), 53-60.</p>
<p>Stanford, J., Tauber, E.R., Fogg, B.J. &amp; Marable, L. (2002). <a href="http://www.consumerwebwatch.org/dynamic/web-credibility-reports-experts-vs-online.cfm">Experts vs. online consumers: A comparative credibility study of health and finance Web sites</a>. <em>Consumer WebWatch Research Report</em>. Retrieved May 8, 2008 from: http://bit.ly/VDWT (Archived by WebCite® at http://www.webcitation.org/5d8h90cZ5)</p>
<p>Strauss, A.L &amp; Corbin, J. (1990). <em>Basics of qualitative research: grounded theory procedures and techniques</em>. London: Sage Publications.</p>
<p>Toms, E.G. &amp; Taves, A.R. (2004). Measuring user perceptions of Web site reputation. <em>Information Processing and Management</em>, <strong>40</strong>(2), 291-317.</p>
<p><a href="http://www.merriam-webster.com/dictionary/truth">Truth.</a> (2007). In <em>Merriam-Webster Online Dictionary</em>. Retrieved 15 December, 2008 from http://www.merriam-webster.com/dictionary/truth (Archived by WebCite® at http://www.webcitation.org/5d8hIHiip)</p>
<p>University of California, Los Angeles. <em>Center for Communication Policy</em>. (2003). <em><a href="http://www.digitalcenter.org/pdf/InternetReportYearThree.pdf">The UCLA Internet report. Surveying the digital future. Year three</a></em>. Retrieved 1 April, 2008 from http://www.digitalcenter.org/pdf/InternetReportYearThree.pdf (Archived by WebCite® at http://www.webcitation.org/5d8grxUZD)</p>
<p>Wang, P. &amp; Soergel, D. (1998). A cognitive model of document use during a research project. Study I: Document selection. <em>Journal of the American Society for Information Science</em>, <strong>49</strong>(2), 115-133.</p>
<p>Wilson, P (1983). <em>Second-hand knowledge: an inquiry into cognitive authority</em>. Westport, CT: Greenwood Press.</p>
</body>
</html>

