#### Vol. 11 No. 2, January 2006



# User satisfaction with referrals at a collaborative virtual reference service

#### [Nahyun Kwon](mailto:nkwon@cas.usf.edu)  
Assistant Professor, School of Library and Information Science,  
College of Arts & Sciences, University of South Florida,  
4202 East Fowler Ave, CIS1040, Tampa, FL 33620-7800, USA  



#### Abstract

> **Introduction.** This study investigated unmonitored referrals in a nationwide, collaborative chat reference service. Specifically, it examined the extent to which questions are referred, the types of questions that are more likely to be referred than others, and the level of user satisfaction with the referrals in the collaborative chat reference service.  
> **Method.** The data analysed for this study were 420 chat reference transaction transcripts along with corresponding online survey questionnaires submitted by the service users. Both sets of data were collected from an electronic archive of a southeastern state public library system that has participated in 24/7 Reference of the Metropolitan Cooperative Library System (MCLS).  
> **Results.** Referrals in the collaborative chat reference service comprised approximately 30% of the total transactions. Circulation-related questions were the most often referred among all question types, possibly because of the inability of 'outside' librarians to access patron accounts. Most importantly, user satisfaction with referrals was found to be significantly lower than that of completed answers.  
> **Conclusion.** The findings of this study addressed the importance of distinguishing two types of referrals: the expert research referrals conducive to collaborative virtual reference services; and the re-directional local referrals that increase unnecessary question traffic, thereby being detrimental to effective use of collaborative reference. Continuing efforts to conceptualize referrals in multiple dimensions are anticipated to fully grasp complex phenomena underlying referrals.

## Introduction

The collaborative virtual reference service is an innovative reference service that can bring many benefits to both service users and libraries. Thanks to collaborations among libraries that share their staff, technology and other resources, no longer are library patrons confined to the services and resources of their local libraries. Patrons now have the opportunity to ask questions of institutions in remote locations for specialized information that would not otherwise be available to them. In many collaborative services such as [24/7 Reference](http://www.247ref.org/aboutus.htm), patrons can access the service without being restricted by time and location. Thus, library patrons could obtain help for their information needs via the help of participating libraries in a reference consortium even when their local libraries are closed.

From the library management perspective, libraries can deploy staff more efficiently by participating in a bigger pool and embrace otherwise more costly virtual reference services. Besides cost reduction, a member library of a consortium could answer difficult subject-based research questions more easily by referring the questions to subject specialists in other libraries in the consortium. However, it seems that a large proportion of questions asked in a collaborative service are simply referred back to the home libraries because they should be resolved at a local level ([Coffman 2002](#Coffman); [Helfer 2003](#Helfer)). Unfortunately, there is little monitoring of further transactions to ensure whether the patrons received satisfying answers.

Unmonitored referrals, originally defined by Dewdney and Ross ([1994](#DewdneyRoss)), refers to a situation where the reference staff gives the patron a call number or refers the patron to a source in the library thought to contain the answer but does not follow up or check to make sure that the source is not only found but actually answers the question. These unmonitored referrals seem to be frequently observed in a collaborative virtual reference setting when 'outside' library staff refer the patrons back to local libraries. If a large proportion of those referrals are made back to the local libraries, however, that would not be an effective allocation of resources.

More importantly, the referrals, if unattended, can easily frustrate the users of collaborative virtual reference services. While reference questions are often answered by _outside_ library staff, many patrons would still view the service as a _local_ service. Whenever the questions require local resources and services to answer adequately, library staff have to refer patrons back to their local libraries to complete the answer. Would the patrons still feel satisfied with a delayed answer? In fact, patrons' frustration caused by being transferred from one point to another in the midst of referrals is not unfamiliar. This frustration is equivalent to our everyday experience from using automated customer services on the phone or asking directions on the street.

Although some studies have attempted to identify the types of questions that are effectively answered in virtual reference, few have examined the types of questions that are more prone to referrals, or user satisfaction with referrals in the context of collaborative virtual reference. Perhaps this is because _referral_ are felt to be legitimate answers, although the notion has yet to be validated. Dewdney and Ross ([1994](#DewdneyRoss)) pointed out the problem of a high proportion of unmonitored referrals (42%) at the physical reference service desk. They maintained that reference staff tend to make unmonitored referrals as a way of circumventing the reference interview. Because few studies have examined the unmonitored referrals in collaborative virtual reference service settings, the present study investigates the extent of referrals and user satisfaction with referrals in these types of services. Specifically, the current study focuses on the following three questions:

Research Question 1: To what extent are questions answered as referrals and to what extent are questions answered completely during collaborative virtual reference sessions?

Research Question 2: Are certain types of questions referred more frequently than others? If so, what types of questions are they?

Research Question 3: Are there any differences in user satisfaction levels betweentransactions answered with referrals and transactions answered with completed answers?

The findings of the study will inform us about the question types that are frequently referred in the collaborative virtual reference service. The results will also inform of the extent to which referrals are made in collaborative virtual reference and the efficacy of the referrals as an acceptable answer category from the users' perspective. It will also help local libraries design their services for optimal question traffic management in the collaborative service.

## Literature review

### Collaborative virtual reference services

Virtual reference services have become a critical component in current reference practice where the intent of libraries is to provide a fully functioning user service in the digital library environment. The rapid growth in the service has been made through both local and national efforts ([Kresh 2002](#Kresh)). Some critics question the cost-effectiveness of virtual reference services because of low usage and high cost ([Coffman 2002](#Coffman); [Coffman & Arret 2004](#CoffmanArret)). Based on a cost-benefit analysis, Coffman and Arret suggest the costs associated with software, training and staffing are 'a pretty expensive way to answer a question' and may not justify continuing the service.

Others argue that virtual reference is a niche service that will merge into a part of reference services just as happened to telephone reference services in the 1930s ([Kern 2004](#Kern)). In particular, responding to the negative view toward virtual references, many suggest joining a consortium as a way for those libraries that cannot afford a stand-alone virtual reference service ([Bailey-Hainer 2005](#Bailey); [Kresh 2002](#Kresh); [Tenopir 2004](#Tenopir)). Examples of collaborative models include an international library network project, the [Collaborative Digital Reference Service](http://www.loc.gov/rr/digiref/) (CDRS), the [CLEVNET Library Consortium](http://cpl.org/clevnet-library-consortium.asp) of Ohio public libraries, and [AskColorado](http://www.askcolorado.org/). Because of many benefits that collaborative service brings to both library management and user services, it is estimated that approximately seventy-six collaborative virtual reference services were in operation through the active participation of libraries of various types and sizes as of November 2004 ([Sloan 2004](#Sloan)).

### Referrals in collaborative virtual references

Reference staff seem to make referrals for their patrons for various reasons. The cases can be divided roughly into two types: expert research and re-directional local referrals. Expert research referrals relate to the situations when the questions are referred to subject specialists who could answer with their high level of expertise and resources. Difficult research questions would fall into this type. The referrals in the other type, re-directional local referrals, relate to the situations when the answers require specific local knowledge or locally-restricted databases. Questions that would fall into this type include questions about local library events that may or may not be listed on the library's Website or patrons' requests to check overdue fines. Despite both being referral, each type of referral could have considerably different implications for reference service delivery. The former type of referral enables member libraries to coordinate human expertise and resources in order to answer the questions that are otherwise unanswerable. In this case, patrons are linked to richer resources and subject specialists.

In contrast, re-directional local referrals can be ineffective or even result in a disservice to the patrons. That is, patrons who access the inter-institutional collaborative network may not necessarily understand the seamless nature of the service that goes beyond the walls of their local library. It is possible that people who access this service through their library portal would ask simple, circulation-related questions (e.g., overdue status), anticipating an immediate answer. However, many times, these questions cannot be completed by _outside_ staff who do not have authority to access patron records. In this case, the patron should be referred back to her or his local library to get the final answer. If a high proportion of the questions submitted to collaborative services are referred to a local library for this latter reason, it becomes an ineffective deployment of resources for the local library. Having this concern, some libraries may decide to remain local for their virtual reference service, understanding that a high proportion of their reference questions relate to their library resources and services ([Kibbee _et al._ 2002](#KibbeeWardMa)).

In the current literature, there are not many empirical studies that have examined the proportion of referrals in reference services. Among the few, Ross and Nilsen ([2000](#RossNilsen)), in their unobtrusive reference service evaluation study, reported that 37% of the total reference transactions were unmonitored referrals. This finding is based on 261 reference transactions that occurred at physical reference desks in both public and academic library settings.

As libraries adopt collaborative virtual reference services, librarians tend to expect more research related referrals because collaboration and communication among institutions have become much easier and seamless in the online network. Interestingly, a rather reverse finding was reported in a recent study by Nilsen ([2004](#Nilsen)) who compared the unmonitored referrals between physical and virtual reference settings. While more than one-third of the transactions have unmonitored referrals in the physical setting, the proportion was somewhat lower in the virtual setting, comprising 28% of a total of 40 transactions. In her updated study with more transactions, unmonitored referrals were reported as 38% of a total of 85 virtual reference transactions where chat reference comprised 25 transactions ([Nilsen 2005](#Nilsen2005)). Since this small sample-size research appears to be the only one that investigated referrals in virtual reference setting, referrals in collaborative virtual reference are still largely unknown.

### Referrals by query type

Are there any specific types of questions that result in more referrals than others in the collaborative virtual reference setting? In other words, would referrals be contingent on the types of questions? Indeed, most factual and subject-based research questions are expected to be answered during the session by most participating librarians, especially when the consortium is equipped with a shared online catalogue and consortia-shared databases. Among these questions, complex subject-based reference questions would be more likely to be referred to subject experts within the consortium. However, this type of question was reported to be very infrequent among the transactions. For example, Lee ([2004](#Lee)) reported that research level questions comprised only 3% of all transactions in a virtual reference environment at an Australian academic library.

A previous study suggests another type of question that would result in high referral rates. According to Kibbee et al. ([2002](#KibbeeWardMa)), a service that generates a high proportion of local questions argues against the inter-institutional collaborative service because the questions would not be effectively answered by _outside_ librarians in the consortium. This is because questions that involve local knowledge (e.g., history, events, or services) or questions that are answerable only by local staff (e.g., use of locally restricted databases or access to patron accounts) tend to result in more referrals back to local libraries. Yet, these speculations should be empirically tested to identify the types of questions that produce more referrals in a collaborative service. The results will serve as important managerial information for designing the practice and policies for virtual reference services.

### User satisfaction with referrals

While the professional community generally encourages the use of referrals as an alternative to actual answers ([Reference and User Services Association 2004](#Reference)), Dewdney and Ross ([1994](#DewdneyRoss)) revealed a pitfall of the referral practice by reporting its ineffectiveness from the users' perspective. In fact, this contention is supported by a collaborative online chat reference use study ([Kwon 2004](#Kwon)). According to the finding, a mere suggestion of alternative resources or making referrals was not a positive predictor of user satisfaction. That is, unmonitored referrals do not warrant user satisfaction because the question might or might not be ultimately answered.

A handful of studies has investigated the impact of referrals on reference effectiveness in traditional reference settings. In the study by Dewdney and Ross ([1994](#DewdneyRoss)), most patrons who received unmonitored referrals reported that they found nothing or very little helpful material when they had to search the sources to which reference staff referred them. In fact, Douglas ([1988](#Douglas)) categorized these referrals as one of the six types of reference service failures. Furthermore, Murfin and Bunge ([1984](#MurfinBunge)) reported that the average reference success rate dropped from 69% to 22% when the librarian was busy and, as a result, made suggestions that were not followed up, instead of actually helping the patron with the search. Murfin and Bunge maintained that the practice, policy or necessity of directing the user rather than accompanying the user on the search was a factor that caused librarians to perform far below their potential. All of the above studies indicate apparent problems in the effectiveness of referrals in the traditional physical reference setting. Yet, there is still a question of how patrons would feel when their questions were referred as opposed to when their questions were answered completely. Furthermore, it is also a question how patrons would feel about referrals in a collaborative virtual reference setting.

While many studies have examined the user satisfaction with virtual reference in general, few researchers have investigated the satisfaction of the patrons whose questions are answered as referrals in the collaborative virtual reference setting. The present study investigated this issue empirically.

## Method

### Setting and participants

The present study examined chat reference services delivered through the Broward County public library system in Florida. As the largest library system in Florida, it has 33 regional and branch libraries. Since August 2002, the system has used the _24/7 Reference_ chat reference service delivered by the Metropolitan Cooperative Library System (MCLS), an association of libraries located in the greater Los Angeles area, funded by a Federal Library Services and Technology Act (LSTA) grant.

The data analysed for the present study were on-line chat reference transactions initiated by the patrons of the Broward Country library system, along with online survey questionnaires, which the service users completed voluntarily. The questionnaire was devised and supplied by _24/7 Reference_ and popped up upon completion of each transaction. While the patrons were mostly the users of the library system, the reference staff who provided the service were from forty-nine library systems across the United States participating in the MCLS 24/7 Reference programme. During the six-month research period between January and June 2004, a total of 1,667 transactions took place and 1,387 were analysable as being either completed or transferred transactions. In order to analyse the influence of referrals on user satisfaction, all 420 transactions that had a corresponding questionnaire that chat service users had submitted immediately after their session to report their satisfaction with the service were chosen. This amounted to 30.1% of the total analysable transactions. By having a chat reference session transcript with a matching user satisfaction survey questionnaire, the researcher was able to examine what question was asked in a reference session, whether the question was answered completely or referred, and to what extent the patron was satisfied with the answer or referrals she or he received.

### Measurement of research variables

#### User satisfaction

User satisfaction was assessed by three indicators: satisfaction with the answer, perceived staff quality, and willingness to use the service again. These indicators were measured by patrons' responses to three questionnaire items. First, satisfaction with the answer was measured by asking the question, 'Were you satisfied with the answer you received to your reference question?' Respondents answered the question by choosing one from 'Yes', 'Not sure' and 'No'. The result shows that, among the 417 respondents who answered this question, 65.2% reported that they were satisfied with the answer received, and 21.1% of the respondents were not sure whether they were satisfied or not. Finally, 12.6% of the respondents reported that they were not satisfied at all.

Perceived staff quality, the second indicator of user satisfaction, was measured by the user response to the question, 'The quality of the library staff service in answering this request was _________'. The answer choices were 'Excellent', 'Good', 'Average' and 'Poor'. According to the finding (N = 416), 68.2% of the respondents answered that the librarians handling the reference questions were 'Excellent'. About 19.5% answered 'Good' and 11.3% answered 'Poor.'

Finally, Willingness to use the service again, the third indicator of user satisfaction, was measured using the questionnaire item, 'Will you use this service again?' The answer choices were 'Very likely', 'Maybe' and 'Never'. 77.2% of users answered that they were very likely to use the service again 19.0% answered maybe, and only 3.8% said they would never use the service again (N = 417).

The overall user satisfaction was computed by summing up the above three indicators ([Footnote 1]). Measurement research literature indicates that composite variables yield scores that are generally more valid and reliable than does a single item ([DeVellis 1991]).

In order to determine the reliability and validity of the scores yielded by the composite variable, a reliability test and a factor analysis were conducted, respectively. As a result, a Cronbach's alpha coefficient of .85 was obtained, indicating that the composite variable generated scores were reliable. The factor analysis revealed that the three items were represented by one factor, with structure-pattern coefficients of .868, .916, and .876\. This factor explained 78.64% of the total variance. This high validity score confirms that the composite variable is measuring a single construct, suggesting that the composite variable is a valid measure of user satisfaction. Finally, the mean of the overall user satisfaction was 12.69 with a standard deviation of 3.44 in the range between a maximum value of 3 for 'highly dissatisfied' and the minimum value of 15 for 'highly satisfied.'

#### Answer completeness

Answer completeness refers to the level of completeness that library staff answers to the patron's inquiry during the reference transaction. A content analysis of 420 online chat reference transaction transcripts was conducted in order to code the levels of answer completeness. A total of 16 different levels of answer completeness were identified from the initial coding (See the first three columns of [Table 1](#Table1)).

Later, the initial sixteen levels of answer completeness were collapsed into four broader categories: completed answer, partial or no answer, referrals, and problematic ending (See the last three columns of [Table 1]). This reduction in the categories helped to remove some ambiguities in the micro-level coding. For example, when a transcript showed an abrupt ending, it was not clear whether it was patron's termination due to delay, librarian's premature ending, or technological failure. Thus, a broader category of 'problematic ending' incorporates all of these possibilities constituting an abrupt ending. A fewer number of answer categories also enables bivariate data analysis that is necessary to answer the research questions. Thus, the final coding for answer completeness categories were defined as follows:

*   A transaction was coded as 'completed answer' if a transcript indicated that the librarian offered the patron direct answers or sources that may include answers to the patron's question and that the librarian ends the transaction with a proper closure (i.e., Librarians asks the patron if the question have been completely answered and the patron responds positively.). Additional sources of information may be provided to the patron along with the answer to the initial question. In particular, in order to determine a proper closure, specific behavioral items that are prescribed in the 'Guidelines for Behavioral Performance of Reference and Information Services Providers' ([2004]) were employed. It is the Guidelines that the Reference and User Services Association (RUSA) developed to train reference staff and to assess their performance. The following four items were specifically selected from the Guidelines:

    > *   4.7: Asks the patrons if additional information is needed after an initial result is found (e.g., 'is there anything else I can help you with?')
    > *   5.1: Asks patrons if their questions have been completely answered (e.g., 'did you find what you needed?,''does this completely answer your question?')
    > *   5.2: Encourages the patrons to return if they have further questions (e.g., 'If you don't find what you are looking for, please come back and we'll try something else.')
    > *   5.9: Takes care not to end the reference interview prematurely.

*   A transaction was coded as 'partial or no answer' if a transcript indicated that the patron's initial question was either partially answered or unanswered and no further reference to other sources or services was provided.
*   A transaction was coded as 'referrals' when a transcript indicated that the patron's initial question was either partially answered or unanswered during the reference transaction but reference staff gave the patron information sources or contact information that may or may not contain the answer. The reference staff did not check if the sources had answered the patron's question.
*   A transaction was coded as 'problematic ending' when a transaction was ended before the patron received the answer due to disconnection, delayed answer, or without clear reason. It also includes librarians' premature ending without proper closing remarks and system failure due to connection problems.

#### Question types

A content analysis of transaction transcripts was undertaken to identify the types of questions that are submitted to the collaborative virtual reference. A total of fourteen categories of question types emerged initially. Again, the fourteen categories were collapsed into five broader question types: simple, factual questions, subject-based research questions, resource access questions, circulation-related questions, and local library information inquiries. Below list the definitions of the five types of questions with examples:

*   Simple, factual question refers to known-item search questions, directional questions, or a question of a factual nature that can be answered quickly by consulting only one or two reference tools ([Garnsey & Powell 2000]). Examples are 'Is the book _Funny Laws and Other Zany Stuff_ by Sheryl Lindsell-Roberts available?', 'What does 'borrower status: restricted' mean?', and 'What is the correct way to write this name, in last name first order: Dr. James R. J. Smith, III?'
*   Subject-based research question refers to a reference question requesting a particular kind and number of books or journal/magazine articles on a specific topic, teaching the steps of the research process, locating hard to find literature, and guidance with database choices ([Marsteller & Mizzy 2003]; [Ward 2004]). Examples are 'Can you tell me how to celebrate children's day in America?', 'I want to know the titles of 5 books that deal with the development of gender identity', and 'Where can I find a current article on the culture of any indigenous tribe?'
*   Resource access question refers to an inquiry about access and use of library catalogues, databases or other library resources. Examples are 'How do I access the Oxford English Dictionary through the library Website?', 'I'm having trouble accessing my library account. Can you help?' and 'Where can I access NetLibrary to read an e-book?'
*   Circulation-related question, for the purposes of this study, refers to an inquiry about circulation policies (e.g., overdues, fines, renewal), online renewals, patron account checking, or other circulation related issues. The answer may or may not be readily found from the Website of the local library. Questions that fall in this category include 'Can you check which books are overdue in my record?', and 'I saw I have a 60 dollar fine. Is there a payment plan or forgiveness policy I could do to be able to check out books again?'
*   Local library information inquiries, for the purposes of this study, refers to a question that involves information about a local library. Questions that fall in this category include 'Can I use a typewriter in Sunrise branch library?' 'I am looking for a job at this library', and 'Where should I e-mail comments about Thursday's 'Balancing the Books' seminar at the main library? There was no box into which we could anonymously drop the evaluation forms'.

#### Inter-coder reliability

Inter-coder reliability tests were undertaken to ensure the consistency of coding for both question types and answer completeness. The tests show the level of agreement between two independent coders. To test the inter-coder reliability, the primary researcher coded the entire 420 transactions. Subsequently, the second coder, a reference librarian who received training for coding the RUSA guideline categories, coded every fifth transaction of the 420 transactions (_n_ = 84). This number comprised 20% of the total transactions, which is a recommended percentage for social science research ([Neuendorf 2002]). A widely used Cohen's Kappa (k) was used as the inter-coder reliability index for this study and was calculated using SPSS 13.0 ([SPSS Inc. 2004]). Cohen's Kappa coefficient was found to be 1.00 for question type and .75 for answer completeness, indicating high agreements between the two coders in classifying both variables.

## Results

### Proportion of referrals and answer completeness

Research Question 1 examines 'To what extent are questions answered as referrals and to what extent are questions completely answered during collaborative virtual reference sessions?' Both Table 1 and Figure 1 show the results of answer completeness based on total transactions (N = 420).

<a name="Table1" id="Table1"></a>

<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd"><caption align="bottom">  
**Table 1: Content analysis of answer completeness**</caption>

<tbody>

<tr>

<th>Initial coding category</th>

<th>Freq.</th>

<th>Percent</th>

<td rowspan="18"> </td>

<th>Final coding</th>

<th>Freq.</th>

<th>Percent</th>

</tr>

<tr>

<td>Answered completely</td>

<td align="center">204</td>

<td align="center">48.6</td>

<td align="center" rowspan="3">Completed answers</td>

<td align="center" rowspan="3">237</td>

<td align="center" rowspan="3">56.4</td>

</tr>

<tr>

<td>Answered & transferred</td>

<td align="center">12</td>

<td align="center">2.9</td>

</tr>

<tr>

<td>Answered & referred</td>

<td align="center">21</td>

<td align="center">5.0</td>

</tr>

<tr>

<td>Ended with partial answer</td>

<td align="center">12</td>

<td align="center">2.9</td>

<td rowspan="2">Partial or no answer</td>

<td align="center" rowspan="2">20</td>

<td align="center" rowspan="2">4.8</td>

</tr>

<tr>

<td>Unanswered or failed</td>

<td align="center">8</td>

<td align="center">1.9</td>

</tr>

<tr>

<td>Part answered & transferred</td>

<td align="center">4</td>

<td align="center">1.0</td>

<td align="center" rowspan="6">Referrals</td>

<td align="center" rowspan="6">122</td>

<td align="center" rowspan="6">29.0</td>

</tr>

<tr>

<td>Part answered & referred</td>

<td align="center">11</td>

<td align="center">2.6</td>

</tr>

<tr>

<td>Unanswered & transferred</td>

<td align="center">6</td>

<td align="center">1.4</td>

</tr>

<tr>

<td>Unanswered & referred</td>

<td align="center">5</td>

<td align="center">1.2</td>

</tr>

<tr>

<td>Referred or suspended</td>

<td align="center">84</td>

<td align="center">20.0</td>

</tr>

<tr>

<td>Transferred</td>

<td align="center">12</td>

<td align="center">2.9</td>

</tr>

<tr>

<td>Patron disconnected before answered</td>

<td align="center">8</td>

<td align="center">1.9</td>

<td rowspan="5">Problematic ending</td>

<td align="center" rowspan="5">41</td>

<td align="center" rowspan="5">9.8</td>

</tr>

<tr>

<td>Uncertain ending</td>

<td align="center">1</td>

<td align="center">0.2</td>

</tr>

<tr>

<td>Disconnect due to delay</td>

<td align="center">19</td>

<td align="center">4.5</td>

</tr>

<tr>

<td>Librarian's premature ending</td>

<td align="center">7</td>

<td align="center">1.7</td>

</tr>

<tr>

<td>System failure</td>

<td align="center">6</td>

<td align="center">1.4</td>

</tr>

<tr>

<td>Total</td>

<td align="center">420</td>

<td align="center">100.0</td>

<td>Total</td>

<td align="center">420</td>

<td align="center">100.0</td>

</tr>

</tbody>

</table>

These findings revealed the answer to Research Question 1; referrals reached up to approximately 30% of all transactions in the collaborative virtual reference service. In contrast to referrals, completely answered transactions comprised 56.4% of the transactions, which is very close to the well-known 55% reference success rate ([Hernon & McClure 1986]; [Saxton & Richardson 2002])

### Question types

A profile of questions that public library patrons asked of collaborative virtual reference was found from a content analysis of transaction transcripts. Figure 1 presents the findings (_N_ = 415).

<div align="center"><a name="Figure1" id="Figure1"></a>![fig2](p246fig2.jpg)</div>

<div align="center">Figure 1: Proportions of five question types (_N_ = 415)  
(*This analysis was based on 415 transactions after eliminating five ambiguous transcripts.)</div>

As shown in Figure 1, circulation-related questions were the most frequently asked questions of all (48.9%), followed by subject-based research questions (25.8%), simple factual questions (9.6%), resource access (8.9%) and local library-related information inquiries (6.8%). It should be noted that the high proportion of circulation-related questions seem to partly attribute to the fact that the circulation page has a link to the service.

### Relationship between question types and answer completeness

Research Question 2 relates to whether certain types of questions are more frequently referred than others. If there are, what types of questions would they be? Figure 2 shows the degree of answer completeness by question types.

<div align="center"><a name="Figure2"></a>![fig3](p246fig3.gif)</div>

<div align="center">Figure 2: Answer completeness by question types (_N_ = 415)</div>

Figure 2 demonstrates that both simple factual and subject-based research questions were completely answered over 70% of the time and their average referral rates were 5% and 11.2% of total transactions, respectively. In the case of circulation and local library information inquiries, answer completion rates were approximately 50% and referral rates were 44.8% and 39.3%, respectively.

To determine a systematic association between answer completeness and question types, a chi-squared test was conducted using a significance level of .05\. According to the test result, there was a statistically significant difference in answer completion rates across the five question types, χ<sup style="font-size: x-small;">2</sup> = 71.616 (12 d.f, _N_ = 415), _p_ < .001\. The effect size ([Footnote 2]) associated with this relationship, as measured by Cramer's V, was .24, which suggests a small to medium effect size. This result indicates that the degree of answer completion is highly correlated with the types of questions. Specifically, this result shows that referrals occurred far more frequently in circulation and local library information inquiries while substantially fewer referrals were made in both simple factual and subject-based research questions.

### User satisfaction with referrals

Research question 3 relates to whether there is any difference in the user satisfaction between referrals and completed answers? To answer this question, the level of user satisfaction was compared across the four answer categories. User satisfaction was highest when the questions were completely answered (Mean = 14.21; SD = 1.88; _n_ = 237), followed by referred (Mean = 11.83; SD = 3.40; _n_ = 122), partial/unanswered (Mean = 10.68; SD = 3.91; _n_ = 20), and problematic endings (Mean= 7.54; SD = 3.87; _n_ = 41). With these results, an analysis of variance (ANOVA) test was conducted to determine if the user satisfaction of referrals is statistically significantly lower than that of completed answers (Figure 3)

<div align="center"><a name="Figure3" id="Fig3"></a>![Fig3](p246fig4.jpg)</div>

<div align="center">  
**Figure 3: User satisfaction by answer completeness (_N_ = 420)**</div>

The ANOVA test results revealed a statistically significant difference in user satisfaction across the four answer categories at the significance level of .05, F(3, 416) = 80.341, p < .001\. A post-hoc test of the overall ANOVA test was conducted to determine how the four groups differ by employing Games-Howell test. The Games-Howell test is considered to be robust when sample sizes and variances are not equal across compared groups ([Field 2005]; [SPSS Inc. 2004]). The test result revealed that user satisfaction was rated into three different levels, as shown in Table 2.

<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd"><caption align="bottom">Table 2: Post hoc ANOVA test using the Games-Howell test</caption>

<tbody>

<tr>

<th width="22%">Answer completeness</th>

<th colspan="3">User satisfaction</th>

</tr>

<tr>

<th> </th>

<th width="25%">High</th>

<th width="21%">Middle</th>

<th width="32%">Low</th>

</tr>

<tr>

<td>Completed answers</td>

<td align="center">14.21 (1.88)*</td>

<td> </td>

<td> </td>

</tr>

<tr>

<td>Referrals</td>

<td> </td>

<td align="center">11.83 (3.40)</td>

<td align="center"> </td>

</tr>

<tr>

<td>Partial or no answer</td>

<td align="center"> </td>

<td align="center">10.68 (3.91)</td>

<td align="center"> </td>

</tr>

<tr>

<td>Problematic Endings</td>

<td align="center"> </td>

<td align="center"> </td>

<td align="center">7.53 (3.87)</td>

</tr>

<tr>

<td colspan="4">*The numbers in parentheses are standard deviations.</td>

</tr>

</tbody>

</table>

As Table 2 shows, patrons who obtained complete answers were most satisfied among all groups. Patrons who received referrals were significantly less satisfied than the patrons whose questions were completely answered; however, patrons who received referrals fell into the same satisfaction level as those whose questions were partially answered or unanswered. More importantly, the current study revealed that users were far less satisfied with the referrals than with completed answers and experience only about the same degree of satisfaction as users receiving either a partial answer or no answer at all. Finally, all three groups were statistically significantly more satisfied than patrons who experienced a problematic ending.

## Discussion

This study investigated referrals in a nationwide, collaborative virtual reference service. Specifically, it examined the extent to which questions are referred in the collaborative virtual reference environment, the types of questions that are more likely to be referred, and whether patrons would be equally satisfied with the referred answers as with complete answers. This study employed both content analysis of the transcripts of 420 virtual reference transactions that occurred between January and June 2004 and corresponding self-reported survey questionnaires that were submitted by the patrons after each transaction.

According to the results of this study, approximately 30% of the questions were answered with referrals. Circulation-related questions were more frequently referred back to the local libraries than simple factual or subject-based research questions, mostly due to restriction of access to patron records. The latter two types of questions are non-local, generic questions in nature. These types of questions tend to be more completely answered than either circulation or local library information inquiries which involve more locality-specific knowledge. These findings indicate that questions involving local resources tend to create more referrals than questions that involve generic knowledge. More importantly, the current study revealed that users were far less satisfied with the referrals than with completed answers and experience only about the same degree of satisfaction as users receiving either a partial answer or no answer at all.

Regarding this problem of lower user satisfaction with referrals, libraries participating in collaborative services could consider several approaches for handling this problem. An immediate measure would be to specify the scope of questions explicitly and effectively on the virtual reference service Web site. As another approach, Braxton and Brunsdale ([2004]) suggested that virtual reference services ask users to check their question types and to describe their questions using a Web form where users can also read the scope of questions available from the service.

Probably a more effective measure to correct the phenomenon of problematic referrals would be to place the links to the virtual reference service in the most natural and pertinent locations, that is, at the point of need. A problem that seems to have contributed to the high volume of circulation-related questions in this study is the location of the link placed on a circulation page. Whenever patrons encountered a circulation-related problem, they seemed to click on the link to get help. However, a large proportion of these circulation-related questions were answerable only by authorized personnel in the patrons' home libraries.

This finding supports Wells's ([2003]) argument that proper location of the links to the virtual reference services can enhance service effectiveness. According to Gray ([2000]), both the placement of links to the services on individual Web pages and the number of links to the service throughout the Web site directly affect the amount of question traffic. All these findings in the literature suggest a possible influence of link locations on the types of questions asked of virtual reference.

However, the results of this study do not support Gray's other recommendation, 'services that provide links on the bottom of all pages, on a main navigation bar, or on a help menu featured throughout the site will provide users with more opportunity to ask questions at the point of need' ([Gray 2000: 369]). According to the findings of this study, not all links placed at the point of need helped the patrons especially when it does not guarantee answers. For instance, the present study reported 45% of questions asked via a link on the circulation page were answered as referrals and therefore patrons had to contact their home library again. These findings also suggest that a link on the circulation page unnecessarily increases circulation question traffic within the collaborative service. As the current findings indicate, user satisfaction with referrals was significantly lower than completed answers. Patrons seemed to have experienced confusion and frustration or felt they had wasted their time in this process. Referrals made back to the local library would work against the anticipated benefits of effective allocation of resources. Regarding this finding, a usability study could be conducted to empirically examine relationships between the locations of links to virtual reference and the service effectiveness.

Despite the apparent problems in some referrals, they seem to be generally encouraged and reference staff seem to believe referrals are legitimate answers to reference questions ([Reference and User Services Association, 2004]). However, the fact that librarians are advised to offer referrals when they do not provide the answer does not necessarily mean that referrals are effective in all circumstances. The findings of this study suggest that librarians should be able to distinguish between two types of referrals: the 'expert research' referrals conducive to collaborative virtual reference services (e.g., sharing expertise and resources in answering difficult subject-based research questions); and the 're-directional, local' referrals that increase unnecessary question traffic, thereby being detrimental to effective use of collaborative reference (e.g., referring patrons back to local libraries to resolve circulation disputes). A practical implication of this finding is that two types of referrals should be properly addressed in reference staff training so that staff can make referrals most effectively. A future study could compare the two types of referrals to further understand contingent conditions of those referrals. In relation to this, another interesting agenda of further research would be to compare the patrons and the reference staff for their perceptions of and attitudes toward referrals.

## Conclusion and implications of the study

While there are many expectations and promises in the emerging collaborative virtual reference service, the present study demonstrated some gaps in the current service delivery by investigating referrals. One limitation of the present study, however, is the fact that the study was conducted at a single public library system although the librarians investigated were from forty-nine different library systems participating in a nationwide collaborative chat reference programme. Thus, the findings of the present study should be further confirmed by replicating the study in libraries of different types and sizes.

As another limitation, it should be noted that the present study included only the transactions that had a completed questionnaire. These transactions comprise only 30% of the total of 1387 analysable transactions. Considering the possibility that the patrons who completed the questionnaire may not represent the entire user population accurately, the findings of the present study should be substantiated by further research.

Despite these limitations, the results of this study provide several implications for future collaborative virtual reference practice and research. First, from the practical perspective, the current study empirically demonstrated that generic reference questions, such as simple factual and subject-based research questions, are more effectively answered than locality-specific questions. These findings can help local libraries make informed decisions on such issues as whether they go global or remain local in providing the service. The study also demonstrated empirically that the patrons did not value referrals to a great extent. This finding suggests that librarians should not regard all referrals as equally good alternatives to completed answers. Most importantly, local libraries can minimize confusion and frustration of their users by evaluating the current locations of links to virtual reference and manage the question traffic to provide proper directions for the patrons. This should also reduce the frustration of reference staff who might have to end their sessions without offering satisfying answers and help them to spend their time and expertise more wisely with the questions that they could better answer.

From the reference research perspective, it can be contended that the present study addressed complex phenomena underlying a seemingly simple behaviour of making referrals. The findings certainly redirect our attention to the concept of 'unmonitored referrals' that Dewdney and Ross ([1994]) had proposed from their research in the physical reference setting. While supporting their claims about the problems in unmonitored referrals, the present study suggests that referrals could bring both positive and negative implications to collaborative virtual reference services. In this regard, the use of referrals must be one such area that Whitlatch ([2004]) pointed out to assure patrons' satisfaction in the current reference service environment where new technology can create both opportunities and threats. Since further understanding of referrals is critical to maximize the user satisfaction in collaborative services, continuing efforts to conceptualize referrals in multiple dimensions are anticipated to fully grasp complex phenomena underlying referrals.

## Notes

<a name="Footnote1" id="Footnote1"></a>1\. In creating a composite variable, a series of computations was conducted because the three items in the survey questionnaire were measured on a three-point or four-point ordinal scale (e.g., 'satisfied,' 'not sure,' and 'not satisfied' for the 'satisfaction with the answer' item). These ordinal level measures are not suitable to undertake inferential statistical tests, such as ANOVA, that are necessary to answer research question 3 of the present study. To resolve this conflict, the existing three questionnaire items were recoded and transformed into a composite variable to increase the variability of the measure. First, the ordinal level data variables were rescaled on a five-point scale as shown below:

<table style="BORDER-RIGHT: solid; BORDER-TOP: solid; FONT-SIZE: smaller; BORDER-LEFT: solid; BORDER-BOTTOM: solid; FONT-STYLE: normal; FONT-FAMILY: Verdana, Geneva, Arial, Helvetica, sans-serif; BACKGROUND-COLOR: #fdffdd" cellspacing="0" cellpadding="3" width="656" align="center" border="1"><caption align="bottom">  
**Adjusted five-point scale**</caption>

<tbody>

<tr>

<th width="35%"> </th>

<th width="3%">Negative</th>

<th width="13%"> </th>

<th width="13%"> </th>

<th width="13%"> </th>

<th width="23%">Positive</th>

</tr>

<tr>

<th> </th>

<th>1</th>

<th>2</th>

<th>3</th>

<th>4</th>

<th>5</th>

</tr>

<tr>

<td>Satisfaction with the answer</td>

<td>Unsatisfied</td>

<td>Not sure</td>

<td> </td>

<td> </td>

<td>Satisfied</td>

</tr>

<tr>

<td>Perceived staff quality</td>

<td>Poor</td>

<td> </td>

<td>Average</td>

<td>Good</td>

<td>Excellent</td>

</tr>

<tr>

<td>Willingness to use the service again</td>

<td>Never</td>

<td> </td>

<td>Maybe</td>

<td> </td>

<td>Very likely</td>

</tr>

</tbody>

</table>

As shown on the above diagram, for satisfaction with the answer item, 1 was assigned for 'unsatisfied,' 2 for 'not sure,' and 5 for 'satisfied.' For perceived staff quality item, 1 was assigned for 'poor,' 3 for 'average,' 4 for 'excellent.' For willingness to return item, 1 was assigned for 'never,' 3 for 'maybe,' and 5 for 'very likely.'

After the three items were rescaled on the same five-point scale, they were summed as a single composite variable, with a minimum value of 3 and a maximum value of 15\. This procedure of data management allowed the researcher to conduct the necessary inferential statistical tests.

<a name="Footnote2" id="Footnote2"></a>

2\. The 'effect size' measured the degree of the strength of the association between the two categorical variables. Many behavioral science research papers now report effect size along with significance test result because of the limitations of the significance test. Effect size is useful information especially if the significance test result is sensitive to the sample size, as with Chi-squared tests. For this paper, I measured the effect size using Cramer's V which is a measure of the effect size of the Chi-squared tests ([Kline 2004]).

## References

*   <a name="Bailey" id="Bailey"></a>Bailey-Hainer, B. (2005). Virtual reference: alive & well. _Library Journal,_ **130**(1),46-47.
*   <a name="Braxton" id="Braxton"></a>Braxton, S.M. & Brunsdale, M. (2004). E-mail reference as substitute for library receptionist. _The Reference Librarian_, **85**(1), 19-31.
*   <a name="Coffman" id="Coffman"></a>Coffman, S. (2002). What's wrong with collaborative digital reference?. _American Libraries_, **33**(11), 56-58.
*   <a name="CoffmanArret" id="CoffmanArret"></a>Coffman, S. & Arret, L. (2004). [To chat or not to chat—taking another look at virtual reference, part I.](http://www.infotoday.com/searcher/jul04/arret_coffman.shtml) _Searcher_, **12**(7), 38-46\. Retrieved 2 August, 2005 from http://www.infotoday.com/searcher/jul04/arret_coffman.shtml
*   <a name="DeVellis" id="DeVellis"></a>DeVellis, R.F. (1991). _Scale development: theory and applications._ Newbury Park, CA: Sage.
*   <a name="DewdneyRoss" id="DewdneyRoss"></a>Dewdney, P. & Ross, C.S. (1994). Flying a light aircraft: reference service evaluation from a user's viewpoint. _RQ_, **34**(2), 217-229.
*   <a name="Douglas" id="Douglas"></a>Douglas, I. (1988). Reducing failures in reference service._RQ_, **28**(1), 94-101.
*   <a name="Field" id="Field"></a>Field, A. (2005). _Discovering statistics using SPSS._ 2nd ed. London: Sage Publications.
*   <a name="Garnsey" id="Garnsey"></a>Garnsey, B.A. & Powell, R.R. (2000). Electronic mail reference services in the public library. _Reference & User Services Quarterly_, **39**(3), 245-254.
*   <a name="Gray" id="Gray"></a>Gray, S.M. (2000). Virtual reference services: directions and agendas. _Reference & User Services Quarterly_, **39**(4), 365-375.
*   <a name="Helfer" id="Helfer"></a>Helfer, D.S. (2003). Virtual reference in libraries: status and issues. _Searcher_, **11**(2), 63-65.
*   <a name="Hernon" id="Hernon"></a>Hernon, P. & McClure, C.R. (1986). Unobtrusive reference testing: the 55 percent rule. _Library Journal_, **111**(7), 37-41.
*   <a name="Kern" id="Kern"></a>Kern, M.K. (2004). Haven't we been here before? lessons from telephone reference. _The Reference Librarian_, **85**(1), 1-17.
*   <a name="KibbeeWardMa" id="KibbeeWardMa"></a>Kibbee, J.Z., Ward, D.H. & Ma, W. (2002). Virtual service, real data: results of a pilot study. _Reference Services Review_, **30**(1), 25-36.
*   <a name="Kline" id="Kline"></a>Kline, R.B. (2004). _Beyond significance testing: reforming data analysis methods in behavioral research._ Washington, DC: American Psychological Association.
*   <a name="Kresh" id="Kresh"></a>Kresh, D.N. (2002). High touch or high tech: the collaborative digital reference service as a model for the future of reference. _Advances in Librarianship_, **26**, 149-173.
*   <a name="Kwon" id="Kwon"></a>Kwon, N. (2004). [_Assessing the virtual reference success using the revised RUSA Guidelines for behavioral performance of reference and information service providers._](http://www.vrd2004.org/proceedings/presentation.cfm?PID=396) Paper presented at the 6th Virtual Reference Desk Conference. Cincinnati, Ohio. November 8-9 2004\. Retrieved 1 August, 2005 from http://www.vrd2004.org/proceedings/presentation.cfm?PID=396
*   <a name="Lee" id="Lee"></a>Lee, I.J. (2004). Do virtual reference librarians dream of digital reference questions?: a qualitative and quantitative analysis of email and chat reference. _Australian Academic & Research Libraries_, **35**(2),95-110.
*   <a name="MarstellerMizzy" id="MarstellerMizzy"></a>Marsteller, M. & Mizzy, D. (2003). Exploring the synchronous digital reference interaction for query types, question negotiation, and patron response. _Internet Reference Service Quarterly_, **8**(1/2), 149-165.
*   <a name="MurfinBunge" id="MurfinBunge"></a>Murfin, M. & Bunge, C. (1984). Evaluating reference service from the patron point of view. _The Reference Librarian_, **11**, 175-82.
*   <a name="Neuendorf" id="Neuendorf"></a>Neuendorf, K.A. (2002). _The content analysis guidebook._ Thousand Oaks, CA: Sage.
*   <a name="Nilsen" id="Nilsen"></a>Nilsen, K. (2004). [The library visit study: user experience at the virtual reference desk.](http://informationr.net/ir/9-2/paper171.html) _Information Research_, **9**(2), paper 171\. Retrieved 2 August, 2005 from http://informationr.net/ir/9-2/paper171.html
*   <a name="Nilsen2005" id="Nilsen2005"></a>Nilsen, K. (2005). [Virtual versus face-to-face reference: comparing users' perspectives on visits to physical and virtual reference desks in public and academic libraries](http://www.ifla.org/IV/ifla71/papers/027e-Nilsen.pdf). In _Proceedings of Libraries: A Voyage of Discovery. World Library and Information Congress, 71st IFLA General Congress and Conference. August 14-18 2005._ Code 71-E, Reference and Information Services, August14 2005\. 2nd version August 24 2005\. Retrieved 25 November, 2005 from http://www.ifla.org/IV/ifla71/papers/027e-Nilsen.pdf
*   <a name="Reference" id="Reference"></a>Reference and User Services Association. (2004). [Guidelines for behavioral performance of reference and information service providers](http://www.ala.org/ala/rusa/rusaprotools/referenceguide/guidelinesbehavioral.htm). Chicago, IL: American Library Association, Reference and User Services Association. Retrieved 2 August 2005 from http://www.ala.org/ala/rusa/rusaprotools/referenceguide/guidelinesbehavioral.htm
*   <a name="RossDewdney" id="RossDewdney"></a>Ross, C.S. & Dewdney, P. (1998). Negative closure: strategies and counter-strategies in the reference transaction. _Reference & User Services Quarterly_, **38**(2), 151-163.
*   <a name="RossNilsen" id="RossNilsen"></a>Ross, C.S. & Nilsen, K. (2000). Has the Internet changed anything in reference? The Library Visit Study Phase 2\. _Reference & User Services Quarterly_, **40**(2), 147-155.
*   <a name="Saxton" id="Saxton"></a>Saxton, M.L., & Richardson, J.V. (2002). Understanding reference transactions: transforming an art into a science. San Diego, CA: Academic Press.
*   <a name="SPSS" id="SPSS"></a>_SPSS 13.0 for Windows_. (2004). Chicago, IL: SPSS Inc.
*   <a name="Sloan" id="Sloan"></a>Sloan, B. (2004). [Collaborative live reference services](http://www.lis.uiuc.edu/~b-sloan/collab.htm). Urbana-Champaign, IL: University of Illinois, Graduate School of Library and Information Science. Retrieved 2 August, 2005 from http://www.lis.uiuc.edu/~b-sloan/collab.htm
*   <a name="Tenopir" id="Tenopir"></a>Tenopir, C. (2004). Rethinking virtual reference. _Library Journal_, **129**(18), 34.
*   <a name="Ward" id="Ward"></a>Ward, D. (2004). Measuring the completeness of reference transactions in online chats: results of an unobtrusive study. _Reference & User Services Quarterly_, **44**(1), 46-56.
*   <a name="Wells" id="Wells"></a>Wells, C.A. (2003). Location, location, location: the importance of placement of the chat request button. _Reference & User Services Quarterly_, **43**(2), 133-137\.
*   <a name="Whitlatch" id="Whitlatch"></a>Whitlatch, J.B. (2004). [Reference futures: outsourcing, the Web, or knowledge counseling](http://www.ala.org/rusa/forums/whitlatch_forum.html ). Chicago, IL: American Library Association, Reference and User Services Association. Retrieved 2 August 2005 from http://www.ala.org/rusa/forums/whitlatch_forum.html

